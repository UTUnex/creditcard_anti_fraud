{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Credit Card Anti Fraud Implementation \n",
    ">1. Project introduction and data source：[Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud)  \n",
    ">2. Machine Learning Category：**Supervised binary classification problem** OR **Unsupervised anamoly detection problem**    \n",
    ">3. Supervised learning scheme 1：Perform the classification task using [BalancedRandomForestClassifier](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.ensemble.BalancedRandomForestClassifier.html#imblearn.ensemble.BalancedRandomForestClassifier), [RusBosstClassifier](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.ensemble.BalancedRandomForestClassifier.html#imblearn.ensemble.BalancedRandomForestClassifier) and [LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html?highlight=classifier)  \n",
    ">4. Supervised learning scheme 2: Also, we can first apply some resampling methods such as [Smotetomek](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.combine.SMOTETomek.html?highlight=smotet#imblearn.combine.SMOTETomek) and [Smoteenn](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.combine.SMOTEENN.html) to make the unbalanced datasets balanced, then we can use the normal classification algorithms as usual  \n",
    ">4. Unsupervised learning scheme：Perform the anamoly detection task using [Isolation Forest](https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=iforest#pyod.models.iforest.IForest) OR [Autoencoder network](https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=iforest#module-pyod.models.auto_encoder)  \n",
    ">5. In this notebook, we are going to implement the [Exploratory data analysis: EDA](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python) and some preprocessing  \n",
    ">5. keywords: [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/index.html),[pytorch](https://pytorch.org/), [skorch](https://skorch.readthedocs.io/en/stable/user/quickstart.html), [tune-sklearn](https://github.com/ray-project/tune-sklearn) from ray.tune, [dask ml](https://ml.dask.org/), [sklearn](https://scikit-learn.org/stable/), [microsoft nni](https://nni.readthedocs.io/en/latest/Overview.html), [hyperopt](https://github.com/hyperopt/hyperopt)  \n",
    ">6. About feature selection: [Borutapy](https://github.com/scikit-learn-contrib/boruta_py) and [GradientFeatureSelector](https://nni.readthedocs.io/en/latest/FeatureEngineering/GradientFeatureSelector.html)  \n",
    "\n",
    "## Tricks to deal with the imbalance  \n",
    ">1. [Oversampling and Undersampling](https://towardsdatascience.com/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb), or the combination of them [Smotetomek](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.combine.SMOTETomek.html?highlight=smotet#imblearn.combine.SMOTETomek) and [Smoteenn](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.combine.SMOTEENN.html)  \n",
    ">2. Evaluation metrics: f1_score, [geometric_mean_score](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.metrics.geometric_mean_score.html#imblearn.metrics.geometric_mean_score)  \n",
    ">* [Ref1:Dealing With Class Imbalanced Datasets For Classification](https://towardsdatascience.com/dealing-with-class-imbalanced-datasets-for-classification-2cc6fad99fd9)    \n",
    ">* [Ref2:机器学习之类别不平衡问题 (3) —— 采样方法](https://www.cnblogs.com/massquantity/p/9382710.html)  \n",
    ">* [Ref3:机器学习中的非均衡问题(imbalanced data)和应对方法](https://zhuanlan.zhihu.com/p/38687978)  \n",
    ">* [Ref4:机器学习：如何解决机器学习中数据不平衡问题](https://www.jianshu.com/p/be343414dd24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Load the data and do some preliminary exploration on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most widely used libraries for data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data of csv format as pandas dataframe\n",
    "data_original = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "pandas.core.frame.DataFrame"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "type(data_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Data overview\n",
    "data_original.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n"
    }
   ],
   "source": [
    "# Show the data scale, dtype of each feature and show whether there exist missing values under each feature\n",
    "data_original.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the info above, we know that:  \n",
    ">1. The dtype of all features are numerical thus we don't have to perform any one hot encodinng which is necessary for categorical features  \n",
    ">2. There are no missing values under each feature thus we don't have to process the missing values like dropping or imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Time      0\nV1        0\nV2        0\nV3        0\nV4        0\nV5        0\nV6        0\nV7        0\nV8        0\nV9        0\nV10       0\nV11       0\nV12       0\nV13       0\nV14       0\nV15       0\nV16       0\nV17       0\nV18       0\nV19       0\nV20       0\nV21       0\nV22       0\nV23       0\nV24       0\nV25       0\nV26       0\nV27       0\nV28       0\nAmount    0\nClass     0\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Recheck the number of missing values under each feature\n",
    "data_original.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Check the total number of missing values in the dataset\n",
    "data_original.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the result above, it's confirmed that there are no missing values in the data thus we won't perform any missing value processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Time      124592\nV1        275663\nV2        275663\nV3        275663\nV4        275663\nV5        275663\nV6        275663\nV7        275663\nV8        275663\nV9        275663\nV10       275663\nV11       275663\nV12       275663\nV13       275663\nV14       275663\nV15       275663\nV16       275663\nV17       275663\nV18       275663\nV19       275663\nV20       275663\nV21       275663\nV22       275663\nV23       275663\nV24       275663\nV25       275663\nV26       275663\nV27       275663\nV28       275663\nAmount     32767\nClass          2\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Count distinct observations under each column including target column\n",
    "data_original.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    284315\n1       492\nName: Class, dtype: int64"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Count unique values under target, i.e., see how many classes in the target column\n",
    "data_original['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Visualize the class distribution using plotly\n",
    "import plotly.express as px\n",
    "fig = px.histogram(data_original,x='Class',color='Class',title='Class Distributions (0: No Fraud || 1: Fraud)')\n",
    "# fig.show()\n",
    "fig.show('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    99.827251\n1     0.172749\nName: Class, dtype: float64"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Show the percentage of each class in the data\n",
    "data_original['Class'].value_counts(normalize = True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                Time            V1            V2            V3            V4  \\\ncount  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \nmean    94813.859575  1.165980e-15  3.416908e-16 -1.373150e-15  2.086869e-15   \nstd     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \nmin         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \nmax    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n\n                 V5            V6            V7            V8            V9  \\\ncount  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \nmean   9.604066e-16  1.490107e-15 -5.556467e-16  1.177556e-16 -2.406455e-15   \nstd    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \nmin   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \nmax    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n\n       ...           V21           V22           V23           V24  \\\ncount  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \nmean   ...  1.656562e-16 -3.444850e-16  2.578648e-16  4.471968e-15   \nstd    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \nmin    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \nmax    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n\n                V25           V26           V27           V28         Amount  \\\ncount  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \nmean   5.340915e-16  1.687098e-15 -3.666453e-16 -1.220404e-16      88.349619   \nstd    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \nmin   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \nmax    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n\n               Class  \ncount  284807.000000  \nmean        0.001727  \nstd         0.041527  \nmin         0.000000  \n25%         0.000000  \n50%         0.000000  \n75%         0.000000  \nmax         1.000000  \n\n[8 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>284807.000000</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>...</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>284807.000000</td>\n      <td>284807.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>94813.859575</td>\n      <td>1.165980e-15</td>\n      <td>3.416908e-16</td>\n      <td>-1.373150e-15</td>\n      <td>2.086869e-15</td>\n      <td>9.604066e-16</td>\n      <td>1.490107e-15</td>\n      <td>-5.556467e-16</td>\n      <td>1.177556e-16</td>\n      <td>-2.406455e-15</td>\n      <td>...</td>\n      <td>1.656562e-16</td>\n      <td>-3.444850e-16</td>\n      <td>2.578648e-16</td>\n      <td>4.471968e-15</td>\n      <td>5.340915e-16</td>\n      <td>1.687098e-15</td>\n      <td>-3.666453e-16</td>\n      <td>-1.220404e-16</td>\n      <td>88.349619</td>\n      <td>0.001727</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>47488.145955</td>\n      <td>1.958696e+00</td>\n      <td>1.651309e+00</td>\n      <td>1.516255e+00</td>\n      <td>1.415869e+00</td>\n      <td>1.380247e+00</td>\n      <td>1.332271e+00</td>\n      <td>1.237094e+00</td>\n      <td>1.194353e+00</td>\n      <td>1.098632e+00</td>\n      <td>...</td>\n      <td>7.345240e-01</td>\n      <td>7.257016e-01</td>\n      <td>6.244603e-01</td>\n      <td>6.056471e-01</td>\n      <td>5.212781e-01</td>\n      <td>4.822270e-01</td>\n      <td>4.036325e-01</td>\n      <td>3.300833e-01</td>\n      <td>250.120109</td>\n      <td>0.041527</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-5.640751e+01</td>\n      <td>-7.271573e+01</td>\n      <td>-4.832559e+01</td>\n      <td>-5.683171e+00</td>\n      <td>-1.137433e+02</td>\n      <td>-2.616051e+01</td>\n      <td>-4.355724e+01</td>\n      <td>-7.321672e+01</td>\n      <td>-1.343407e+01</td>\n      <td>...</td>\n      <td>-3.483038e+01</td>\n      <td>-1.093314e+01</td>\n      <td>-4.480774e+01</td>\n      <td>-2.836627e+00</td>\n      <td>-1.029540e+01</td>\n      <td>-2.604551e+00</td>\n      <td>-2.256568e+01</td>\n      <td>-1.543008e+01</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>54201.500000</td>\n      <td>-9.203734e-01</td>\n      <td>-5.985499e-01</td>\n      <td>-8.903648e-01</td>\n      <td>-8.486401e-01</td>\n      <td>-6.915971e-01</td>\n      <td>-7.682956e-01</td>\n      <td>-5.540759e-01</td>\n      <td>-2.086297e-01</td>\n      <td>-6.430976e-01</td>\n      <td>...</td>\n      <td>-2.283949e-01</td>\n      <td>-5.423504e-01</td>\n      <td>-1.618463e-01</td>\n      <td>-3.545861e-01</td>\n      <td>-3.171451e-01</td>\n      <td>-3.269839e-01</td>\n      <td>-7.083953e-02</td>\n      <td>-5.295979e-02</td>\n      <td>5.600000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>84692.000000</td>\n      <td>1.810880e-02</td>\n      <td>6.548556e-02</td>\n      <td>1.798463e-01</td>\n      <td>-1.984653e-02</td>\n      <td>-5.433583e-02</td>\n      <td>-2.741871e-01</td>\n      <td>4.010308e-02</td>\n      <td>2.235804e-02</td>\n      <td>-5.142873e-02</td>\n      <td>...</td>\n      <td>-2.945017e-02</td>\n      <td>6.781943e-03</td>\n      <td>-1.119293e-02</td>\n      <td>4.097606e-02</td>\n      <td>1.659350e-02</td>\n      <td>-5.213911e-02</td>\n      <td>1.342146e-03</td>\n      <td>1.124383e-02</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>139320.500000</td>\n      <td>1.315642e+00</td>\n      <td>8.037239e-01</td>\n      <td>1.027196e+00</td>\n      <td>7.433413e-01</td>\n      <td>6.119264e-01</td>\n      <td>3.985649e-01</td>\n      <td>5.704361e-01</td>\n      <td>3.273459e-01</td>\n      <td>5.971390e-01</td>\n      <td>...</td>\n      <td>1.863772e-01</td>\n      <td>5.285536e-01</td>\n      <td>1.476421e-01</td>\n      <td>4.395266e-01</td>\n      <td>3.507156e-01</td>\n      <td>2.409522e-01</td>\n      <td>9.104512e-02</td>\n      <td>7.827995e-02</td>\n      <td>77.165000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>172792.000000</td>\n      <td>2.454930e+00</td>\n      <td>2.205773e+01</td>\n      <td>9.382558e+00</td>\n      <td>1.687534e+01</td>\n      <td>3.480167e+01</td>\n      <td>7.330163e+01</td>\n      <td>1.205895e+02</td>\n      <td>2.000721e+01</td>\n      <td>1.559499e+01</td>\n      <td>...</td>\n      <td>2.720284e+01</td>\n      <td>1.050309e+01</td>\n      <td>2.252841e+01</td>\n      <td>4.584549e+00</td>\n      <td>7.519589e+00</td>\n      <td>3.517346e+00</td>\n      <td>3.161220e+01</td>\n      <td>3.384781e+01</td>\n      <td>25691.160000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Show some statistics of the dataset\n",
    "data_original.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the data exploration:  \n",
    "1. There are in total 284807 samples in the dataset  \n",
    "2. Data contains 30 feature columns and 1 target column \n",
    "3. There are no missing values under each feature column and target column so no need for missing value processing\n",
    "4. All the features are continuous numerical features without categorical features,so no need for one-hot encoding\n",
    "5. The class distribution is clearly quite imbalanced, with no-fraud occupying 99.83% and fraud occupying only 0.17%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Feature-label split and train-test split  \n",
    ">To avoid the data leakage，before any data preprocessing, perform the train-test split first  \n",
    ">[Reference1：Normalize data before or after split of training and testing data?](https://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data)  \n",
    ">[Reference2：Onehotencoding before or after split of training and testing data?](https://stackoverflow.com/questions/55525195/do-i-have-to-do-one-hot-encoding-separately-for-train-and-test-dataset)  \n",
    ">[Reference3：Imputation before or after train test spliting](https://stats.stackexchange.com/questions/95083/imputation-before-or-after-splitting-into-train-and-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-label split\n",
    "# we think the 'time' feature is not that influential so we just delete it\n",
    "X = data_original.iloc[:,1:-1]\n",
    "y = data_original['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         V1        V2        V3        V4        V5        V6        V7  \\\n0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9       V10  ...       V20       V21       V22       V23  \\\n0  0.098698  0.363787  0.090794  ...  0.251412 -0.018307  0.277838 -0.110474   \n1  0.085102 -0.255425 -0.166974  ... -0.069083 -0.225775 -0.638672  0.101288   \n2  0.247676 -1.514654  0.207643  ...  0.524980  0.247998  0.771679  0.909412   \n3  0.377436 -1.387024 -0.054952  ... -0.208038 -0.108300  0.005274 -0.190321   \n4 -0.270533  0.817739  0.753074  ...  0.408542 -0.009431  0.798278 -0.137458   \n\n        V24       V25       V26       V27       V28  Amount  \n0  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \n1 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n3 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50  \n4  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99  \n\n[5 rows x 29 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>...</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>0.090794</td>\n      <td>...</td>\n      <td>0.251412</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>-0.166974</td>\n      <td>...</td>\n      <td>-0.069083</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>0.207643</td>\n      <td>...</td>\n      <td>0.524980</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>-0.054952</td>\n      <td>...</td>\n      <td>-0.208038</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>0.753074</td>\n      <td>...</td>\n      <td>0.408542</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 29 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    0\n1    0\n2    0\n3    0\n4    0\nName: Class, dtype: int64"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split with 75% data split as train set and 25% as test set\n",
    "# Attention，here we'll use the stratified sampling for the dataset split to confirm that the class distribution keeps same or  similar before and after splitting\n",
    "# Besides，if we already know certaian feature is crucial beforeband, we can also perform the stratified sampling based on that feature instead of the class\n",
    "# Reference1：https://medium.com/@411.codebrain/train-test-split-vs-stratifiedshufflesplit-374c3dbdcc36\n",
    "# Reference2：https://zhuanlan.zhihu.com/p/49991313\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42) "
   ]
  },
  {
   "source": [
    "Calculate class weights  \n",
    ">1. For binary classification problems, some models may require a class_weight of dict format: {0:0.2, 1:0.6}, while the other models may require a scale_pos_weight = count(negative examples)/count(Positive examples)  \n",
    ">2. For multiclassification problems, some models require a dict class_weieght while others may require a tensor  \n",
    ">3. To calculate the class weights for each class, one way is to do it following the rules :  \n",
    "Weight of class C is the size of largest class divided by the size of class C  \n",
    ">4. Another more convenient way is to utilize the sklearn __compute_class_weight function__, however the result of this function is an array not a dict, thus we still need to transfer it to a dict format using dict(enumerate())\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this is a imbalanced problem ,we need calculate the weights of each class for the following pytorch training\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "# Important : https://datascience.stackexchange.com/questions/48369/what-loss-function-to-use-for-imbalanced-classes-using-pytorch\n",
    "# https://blog.csdn.net/oJiMoDeYe12345/article/details/80196376\n",
    "weight_torch = torch.tensor([1,577.88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the second way which may be more convenient to calculate class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "type(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{0: 0.5008652385150725, 1: 289.43766937669375}"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "cw = dict(enumerate(class_weights))\n",
    "cw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.Feature Scaling  \n",
    ">1. Why should we perform the feature scaling?  \n",
    "*After feature scaling，the magnitude of each feature will be similar which help accelerate gradient descent converge*  \n",
    "*[Ref1:Feature Scaling with scikit-learn](https://benalexkeen.com/feature-scaling-with-scikit-learn/)*  \n",
    "*[Ref2:为什么 feature scaling 会使 gradient descent 的收敛更好?](https://www.zhihu.com/question/37129350)*  \n",
    "*[Ref3:机器学习（一）- feature scaling](https://blog.csdn.net/mike112223/article/details/74923096)*  \n",
    "*[Ref4:sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) *  \n",
    "*[Ref5:特征缩放的几种方法](https://www.cnblogs.com/HuZihu/p/9761161.html) *  \n",
    ">2. *We only perform the feature scaling on those continuous numeric features，not on any discrete categorical features*  \n",
    "*[Ref1:Should you ever standardise binary variables?](https://stats.stackexchange.com/questions/59392/should-you-ever-standardise-binary-variables)*  \n",
    "*[Ref2:Dummy variables, is it necessary to standardize them?\n",
    "](https://stackoverflow.com/questions/50557129/dummy-variables-is-necessary-to-standardize-them)*  \n",
    ">3. *Three most widely used feature scaling methods: min-max scaling，standardization and RobustScaler:*  \n",
    "   *- min-max scaling(normalization):*  \n",
    "   $ X_{minmax} = \\frac{X - X_{min}}{X_{max} - X_{min}} $  \n",
    "   *- standardization(Z-score):*  \n",
    "   $ X_{std} = \\frac{X - \\mu }{\\sigma } $  *($\\mu$ is the mean, $\\sigma$ is the standard deviation)*  \n",
    "   *- robustscaler:*  \n",
    "   $ X_{rb} = \\frac{X - Q_1(x) }{Q_3(x) - Q_1(x)} $ *($Q_1$ is the first Quartile， $Q_3$ is the third Quartile)*  \n",
    "   [Ref1:Feature Scaling with scikit-learn](https://benalexkeen.com/feature-scaling-with-scikit-learn/) \n",
    ">4. *Other feature scaling methods: PowerTransformer and QuantileTransformer*\n",
    ">5. *[Ref1：Compare the effect of different scalers on data with outliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py) *  \n",
    "*[Ref2：Which advantages does MinMax scaling have over a standard scaling using the mean and the standard deviation?](https://www.quora.com/Which-advantages-does-MinMax-scaling-have-over-a-standard-scaling-using-the-mean-and-the-standard-deviation) *  \n",
    "*[Ref3：How to Identify Outliers in your Data？](https://machinelearningmastery.com/how-to-identify-outliers-in-your-data/) *  \n",
    "*[Ref4：How to Make Your Machine Learning Models Robust to Outliers？](https://www.kdnuggets.com/2018/08/make-machine-learning-models-robust-outliers.html) *      \n",
    ">6. *__Attention__: We need first split the train and test set, then perform the feature scaling on train set, then perform the feature scaling on test set, with the scaler values calculated on train set (not test set).*  \n",
    "*[Ref1：Difference between preprocessing train and test set before and after splitting](https://stats.stackexchange.com/questions/267012/difference-between-preprocessing-train-and-test-set-before-and-after-splitting) *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the Standardization(Z-score) method which has been widely used for NN feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_std = StandardScaler()  \n",
    "X_train_copy = X_train.copy(deep=True)\n",
    "X_train_std = scaler_std.fit_transform(X_train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "type(X_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "pandas.core.series.Series"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "source": [
    "### IV. Feature Selection  \n",
    ">* Essence of feature selection：drop those redundant or irrelevant features, perform the dimensionality reduction on the orignal feature space \n",
    ">* possible benefit of feature selection：improve the model's predictability, avoid overfitting, improve the training speed  \n",
    ">* Location of feature selection: generally it should be done [after the feature selection](https://stackoverflow.com/questions/46062679/right-order-of-doing-feature-selection-pca-and-normalization)  \n",
    ">* Feature selection methods: generally, three categories (Filter)，(Wrapper) and (Embedded)  \n",
    ">>* Filter method doesn't rely on any ML model. With this method, we don't have to train some model to calculate the feature importances, so it's model-independent. Some widely used methods in this category are:  \n",
    "Welch's t-Test, Fisher score, Chi-Squared test, Relief-based, CFS(Correlation-based feature selection),  \n",
    "FCBF(Fast correlation-based filter)等等。\n",
    ">>* Wrapper method needs a certain model to train and then based on the results, it gives the feature importances. The problem is, there are no guildlines on which model to choose to train for the feature selection, so it's model dependent. To solve this model uncertainty, there are two possible ways that can be tried. First, use the voting method, which means we can first try several models for feature selection and finally based on the voting rules to confirm which features should be left. [Second, keep the model for feature selection and the model for the final training consistent](https://zhuanlan.zhihu.com/p/74198735). Possible wrapper methods：Recursive Feature Elimination(RFE), Simulated Annealing(SA), and Genetic Algorithm(GA) etc. Among them，[genetic algorithm can confirm to get the global optimum](http://www.feat.engineering/genetic-algorithms.html),but it's very time-consuming，low efficient，and is advised to try only when the computing resource is quite sufficient and you don't have to catch the deadline.  \n",
    ">>* Embedded method，regularization, L1-Lasso，L2-Ridge and Elastic Net.\n",
    ">* [Ref1:Overview of feature selection methods](https://towardsdatascience.com/overview-of-feature-selection-methods-a2d115c7a8f7)  \n",
    "[Ref2:Machine Learning Explained: Regularization](http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/)  \n",
    "[Ref3:机器学习（六）：特征选择方法—Filter,Wrapper,Embedded](https://zhuanlan.zhihu.com/p/120924870)  \n",
    "[Ref4:基于 Jupyter 的特征工程手册：特征选择（四）](https://zhuanlan.zhihu.com/p/138758083)  \n",
    "[Ref5：特征选择，经典三刀](https://zhuanlan.zhihu.com/p/24635014)  \n",
    "[Ref6：特征工程笔记](https://www.jianshu.com/p/03284dd5e0bf)  \n",
    "\n",
    ">* Two promising feature selection methods:  \n",
    "[Borutapy](https://github.com/scikit-learn-contrib/boruta_py) and [GradientFeatureSelector](https://nni.readthedocs.io/en/latest/FeatureEngineering/GradientFeatureSelector.html)：  \n",
    ">>* When using BorutaPy, in the wrapper we should select the RandomForest or BalancedRandomForest for feature selection. Some other models like xgboost and lightgbm are not compatible.  \n",
    ">* [Boruta Ref1：Select Important Variables using Boruta Algorithm](https://www.datasciencecentral.com/profiles/blogs/select-important-variables-using-boruta-algorithm)  \n",
    ">* [Boruta Ref2：Boruta explained exactly how you wished someone explained to you](https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a)  \n",
    ">* [Boruta Ref3：Feature selection? You are probably doing it wrong](https://towardsdatascience.com/feature-selection-you-are-probably-doing-it-wrong-985679b41456)  \n",
    ">* [Boruta Ref4: Boruta-xgboost](https://github.com/chasedehan/BoostARoota)  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],\n      dtype=object)"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# Return the feature names of the dataset\n",
    "# https://stackoverflow.com/questions/19482970/get-list-from-pandas-dataframe-column-headers\n",
    "Columns = X_train.columns.to_numpy()\n",
    "Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth value is advised to be set within the range randint [3,7] by the author of the botutapy\n",
    "# the other hyparams just keep the default setting, no tuning here\n",
    "brf = BalancedRandomForestClassifier(n_jobs=-1, class_weight = cw, max_depth=7,random_state=42)\n",
    "feat_selector_brf = BorutaPy(brf, n_estimators='auto', verbose=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iteration: \t1 / 100\nConfirmed: \t0\nTentative: \t29\nRejected: \t0\nIteration: \t2 / 100\nConfirmed: \t0\nTentative: \t29\nRejected: \t0\nIteration: \t3 / 100\nConfirmed: \t0\nTentative: \t29\nRejected: \t0\nIteration: \t4 / 100\nConfirmed: \t0\nTentative: \t29\nRejected: \t0\nIteration: \t5 / 100\nConfirmed: \t0\nTentative: \t29\nRejected: \t0\nIteration: \t6 / 100\nConfirmed: \t0\nTentative: \t29\nRejected: \t0\nIteration: \t7 / 100\nConfirmed: \t0\nTentative: \t29\nRejected: \t0\nIteration: \t8 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t20\nIteration: \t9 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t20\nIteration: \t10 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t20\nIteration: \t11 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t20\nIteration: \t12 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t13 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t14 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t15 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t16 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t17 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t18 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t19 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t20 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t21 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t22 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t23 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t24 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t25 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t26 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t27 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t28 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t29 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t30 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t31 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t32 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t33 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t34 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t35 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t36 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t37 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t38 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t39 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t40 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t41 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t42 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t43 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t44 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t45 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t46 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t47 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t48 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t49 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t50 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t51 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t52 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t53 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t54 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t55 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t56 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t57 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t58 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t59 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t60 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t61 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t62 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t63 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t64 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t65 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t66 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t67 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t68 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t69 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t70 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t71 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t72 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t73 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t74 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t75 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t76 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t77 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t78 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t79 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t80 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t81 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t82 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t83 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t84 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t85 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t86 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t87 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t88 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t89 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t90 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t91 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t92 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t93 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t94 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t95 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t96 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t97 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t98 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\nIteration: \t99 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\n\n\nBorutaPy finished running.\n\nIteration: \t100 / 100\nConfirmed: \t7\nTentative: \t1\nRejected: \t21\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BorutaPy(estimator=BalancedRandomForestClassifier(class_weight={0: 0.5008652385150725,\n                                                                1: 289.43766937669375},\n                                                  max_depth=7, n_estimators=57,\n                                                  n_jobs=-1,\n                                                  random_state=RandomState(MT19937) at 0x20F07464D40),\n         n_estimators='auto',\n         random_state=RandomState(MT19937) at 0x20F07464D40, verbose=2)"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "feat_selector_brf.fit(X_train_std, y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['V1', 'V3', 'V4', 'V11', 'V12', 'V14', 'V24'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# After feature selection by Borutapy, the original features will be categorized as three classes: (1) those that are confirmed useful for model prediction and so kept (2) those that are confirmed not useful and so deleted (3) those that Boruta cannot confirm wether useful or not so cannot decide whether to keep or delete and this decision should be made by users, in short, those that are in doubt\n",
    "# support_ attribute contains the first class while support_weak_ contains the third class\n",
    "confirmed_brf = feat_selector_brf.support_\n",
    "features_confirmed_brf = Columns[confirmed_brf]\n",
    "features_confirmed_brf"
   ]
  },
  {
   "source": [
    "So, Borutapy think the features \\['V1', 'V3', 'V4', 'V11', 'V12', 'V14', 'V24'\\] are useful and will keep them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['V9'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "tentative_brf = feat_selector_brf.support_weak_\n",
    "features_tentative_brf = Columns[tentative_brf]\n",
    "features_tentative_brf"
   ]
  },
  {
   "source": [
    "Similarly, Borutapy think whether to keep or delete the feature \\['V9'\\] is tentative and needs user's decision"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(213605, 7)"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# features_filtered_narrow only keep those confirmed features by Borutapy\n",
    "X_train_filtered_brf_narrow = feat_selector_brf.transform(X_train_std)\n",
    "X_train_filtered_brf_narrow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(213605, 8)"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# features_filtered_broad contains both the confirmed and tentative features,just set weak = True\n",
    "X_train_filtered_brf_broad = feat_selector_brf.transform(X_train_std,weak = True)\n",
    "X_train_filtered_brf_broad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600203439617",
   "display_name": "Python 3.8.5 64-bit ('ml': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}