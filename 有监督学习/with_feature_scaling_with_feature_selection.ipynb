{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信用卡反欺诈检测之基于imbalanced-learn,XGBoost和LightGBM的有监督学习实现  \n",
    ">1. 数据及项目来源：[Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud)  \n",
    ">2. 问题类别：**有监督学习的二分类问题**或者是**无监督学习的异常检测问题**    \n",
    ">3. 有监督学习方案：使用imbalanced-learn中的BalancedRandomForestClassifier,RUSBoostClassifier以及XGBoost和LightGBM四种模型对数据进行分类  \n",
    ">4. 无监督学习方案：使用Isolation Forest（孤立森林）对数据进行异常检测  \n",
    "\n",
    ">5. 思路：单一变量原则，逐渐叠加影响因子  \n",
    ">>1. 首先对未进行特征缩放的数据进行训练和测试，查看结果，文件：without_feature_scaling_without_feature_selection.ipynb  \n",
    ">>2. 然后对经过特征缩放但未经过特征选择的数据进行训练和测试，查看结果  文件：with_feature_scaling_without_feature_selection.ipynb  \n",
    ">>>* 有关特征缩放对本项目使用的四种基于决策树的模型的影响，请参阅本notebook末尾的结果分析部分  \n",
    ">>3. 最后对经过特征缩放和特征选择的数据进行训练和测试，查看结果  文件：with_feature_scaling_with_feature_selection.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. 加载数据并对其进行初步的探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据前处理的通用库numpy和pandas\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据为Pandas dataframe格式\n",
    "data_original = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "pandas.core.frame.DataFrame"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#数据为Pandas dataframe格式\n",
    "type(data_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n5  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427 -0.232794   \n6  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104 -0.780055  0.750137   \n7 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709 -0.415267   \n8  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233  1.011592  0.373205   \n9  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794 -0.385050 -0.069733   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n5  0.105915  0.253844  0.081080    3.67      0  \n6 -0.257237  0.034507  0.005168    4.99      0  \n7 -0.051634 -1.206921 -1.085339   40.80      0  \n8 -0.384157  0.011747  0.142404   93.20      0  \n9  0.094199  0.246219  0.083076    3.68      0  \n\n[10 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.0</td>\n      <td>-0.425966</td>\n      <td>0.960523</td>\n      <td>1.141109</td>\n      <td>-0.168252</td>\n      <td>0.420987</td>\n      <td>-0.029728</td>\n      <td>0.476201</td>\n      <td>0.260314</td>\n      <td>-0.568671</td>\n      <td>...</td>\n      <td>-0.208254</td>\n      <td>-0.559825</td>\n      <td>-0.026398</td>\n      <td>-0.371427</td>\n      <td>-0.232794</td>\n      <td>0.105915</td>\n      <td>0.253844</td>\n      <td>0.081080</td>\n      <td>3.67</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>4.0</td>\n      <td>1.229658</td>\n      <td>0.141004</td>\n      <td>0.045371</td>\n      <td>1.202613</td>\n      <td>0.191881</td>\n      <td>0.272708</td>\n      <td>-0.005159</td>\n      <td>0.081213</td>\n      <td>0.464960</td>\n      <td>...</td>\n      <td>-0.167716</td>\n      <td>-0.270710</td>\n      <td>-0.154104</td>\n      <td>-0.780055</td>\n      <td>0.750137</td>\n      <td>-0.257237</td>\n      <td>0.034507</td>\n      <td>0.005168</td>\n      <td>4.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>7.0</td>\n      <td>-0.644269</td>\n      <td>1.417964</td>\n      <td>1.074380</td>\n      <td>-0.492199</td>\n      <td>0.948934</td>\n      <td>0.428118</td>\n      <td>1.120631</td>\n      <td>-3.807864</td>\n      <td>0.615375</td>\n      <td>...</td>\n      <td>1.943465</td>\n      <td>-1.015455</td>\n      <td>0.057504</td>\n      <td>-0.649709</td>\n      <td>-0.415267</td>\n      <td>-0.051634</td>\n      <td>-1.206921</td>\n      <td>-1.085339</td>\n      <td>40.80</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>7.0</td>\n      <td>-0.894286</td>\n      <td>0.286157</td>\n      <td>-0.113192</td>\n      <td>-0.271526</td>\n      <td>2.669599</td>\n      <td>3.721818</td>\n      <td>0.370145</td>\n      <td>0.851084</td>\n      <td>-0.392048</td>\n      <td>...</td>\n      <td>-0.073425</td>\n      <td>-0.268092</td>\n      <td>-0.204233</td>\n      <td>1.011592</td>\n      <td>0.373205</td>\n      <td>-0.384157</td>\n      <td>0.011747</td>\n      <td>0.142404</td>\n      <td>93.20</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>9.0</td>\n      <td>-0.338262</td>\n      <td>1.119593</td>\n      <td>1.044367</td>\n      <td>-0.222187</td>\n      <td>0.499361</td>\n      <td>-0.246761</td>\n      <td>0.651583</td>\n      <td>0.069539</td>\n      <td>-0.736727</td>\n      <td>...</td>\n      <td>-0.246914</td>\n      <td>-0.633753</td>\n      <td>-0.120794</td>\n      <td>-0.385050</td>\n      <td>-0.069733</td>\n      <td>0.094199</td>\n      <td>0.246219</td>\n      <td>0.083076</td>\n      <td>3.68</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#概览数据，显示前10行\n",
    "data_original.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\nTime      284807 non-null float64\nV1        284807 non-null float64\nV2        284807 non-null float64\nV3        284807 non-null float64\nV4        284807 non-null float64\nV5        284807 non-null float64\nV6        284807 non-null float64\nV7        284807 non-null float64\nV8        284807 non-null float64\nV9        284807 non-null float64\nV10       284807 non-null float64\nV11       284807 non-null float64\nV12       284807 non-null float64\nV13       284807 non-null float64\nV14       284807 non-null float64\nV15       284807 non-null float64\nV16       284807 non-null float64\nV17       284807 non-null float64\nV18       284807 non-null float64\nV19       284807 non-null float64\nV20       284807 non-null float64\nV21       284807 non-null float64\nV22       284807 non-null float64\nV23       284807 non-null float64\nV24       284807 non-null float64\nV25       284807 non-null float64\nV26       284807 non-null float64\nV27       284807 non-null float64\nV28       284807 non-null float64\nAmount    284807 non-null float64\nClass     284807 non-null int64\ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n"
    }
   ],
   "source": [
    "#显示数据规模，各个特征的数据类型；查看各个特征下是否存在缺失值Null\n",
    "#根据Kaggle上该数据集的描述，以及本条代码的查看结果，该数据的所有特征均为数值类型，并且没有缺失值，因此，不需要进行one-hot编码，也不需要进行缺失值处理\n",
    "data_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Time      0\nV1        0\nV2        0\nV3        0\nV4        0\nV5        0\nV6        0\nV7        0\nV8        0\nV9        0\nV10       0\nV11       0\nV12       0\nV13       0\nV14       0\nV15       0\nV16       0\nV17       0\nV18       0\nV19       0\nV20       0\nV21       0\nV22       0\nV23       0\nV24       0\nV25       0\nV26       0\nV27       0\nV28       0\nAmount    0\nClass     0\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#统计各个特征下Null的数量，经过查看，我们发现该数据的确不存在缺失值，与info()的结果一致\n",
    "data_original.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "#显示整个数据集中缺失值Null的总数\n",
    "data_original.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Time      124592\nV1        275663\nV2        275663\nV3        275663\nV4        275663\nV5        275663\nV6        275663\nV7        275663\nV8        275663\nV9        275663\nV10       275663\nV11       275663\nV12       275663\nV13       275663\nV14       275663\nV15       275663\nV16       275663\nV17       275663\nV18       275663\nV19       275663\nV20       275663\nV21       275663\nV22       275663\nV23       275663\nV24       275663\nV25       275663\nV26       275663\nV27       275663\nV28       275663\nAmount     32767\nClass          2\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#显示各个特征包含的唯一值的数量\n",
    "data_original.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    284315\n1       492\nName: Class, dtype: int64"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "#显示数据集中各个类别的数量（标签的数量），我们发现两个类别的分布极不平衡\n",
    "data_original['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    99.827251\n1     0.172749\nName: Class, dtype: float64"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "#显示数据集中各个类别的百分比（标签的百分比），再次验证这是一个类别分布极不平衡的数据集\n",
    "data_original['Class'].value_counts(normalize = True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                Time            V1            V2            V3            V4  \\\ncount  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \nmean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \nstd     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \nmin         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \nmax    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n\n                 V5            V6            V7            V8            V9  \\\ncount  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \nmean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \nstd    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \nmin   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \nmax    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n\n       ...           V21           V22           V23           V24  \\\ncount  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \nmean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \nstd    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \nmin    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \nmax    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n\n                V25           V26           V27           V28         Amount  \\\ncount  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \nmean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \nstd    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \nmin   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \nmax    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n\n               Class  \ncount  284807.000000  \nmean        0.001727  \nstd         0.041527  \nmin         0.000000  \n25%         0.000000  \n50%         0.000000  \n75%         0.000000  \nmax         1.000000  \n\n[8 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>count</td>\n      <td>284807.000000</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>...</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>284807.000000</td>\n      <td>284807.000000</td>\n    </tr>\n    <tr>\n      <td>mean</td>\n      <td>94813.859575</td>\n      <td>3.919560e-15</td>\n      <td>5.688174e-16</td>\n      <td>-8.769071e-15</td>\n      <td>2.782312e-15</td>\n      <td>-1.552563e-15</td>\n      <td>2.010663e-15</td>\n      <td>-1.694249e-15</td>\n      <td>-1.927028e-16</td>\n      <td>-3.137024e-15</td>\n      <td>...</td>\n      <td>1.537294e-16</td>\n      <td>7.959909e-16</td>\n      <td>5.367590e-16</td>\n      <td>4.458112e-15</td>\n      <td>1.453003e-15</td>\n      <td>1.699104e-15</td>\n      <td>-3.660161e-16</td>\n      <td>-1.206049e-16</td>\n      <td>88.349619</td>\n      <td>0.001727</td>\n    </tr>\n    <tr>\n      <td>std</td>\n      <td>47488.145955</td>\n      <td>1.958696e+00</td>\n      <td>1.651309e+00</td>\n      <td>1.516255e+00</td>\n      <td>1.415869e+00</td>\n      <td>1.380247e+00</td>\n      <td>1.332271e+00</td>\n      <td>1.237094e+00</td>\n      <td>1.194353e+00</td>\n      <td>1.098632e+00</td>\n      <td>...</td>\n      <td>7.345240e-01</td>\n      <td>7.257016e-01</td>\n      <td>6.244603e-01</td>\n      <td>6.056471e-01</td>\n      <td>5.212781e-01</td>\n      <td>4.822270e-01</td>\n      <td>4.036325e-01</td>\n      <td>3.300833e-01</td>\n      <td>250.120109</td>\n      <td>0.041527</td>\n    </tr>\n    <tr>\n      <td>min</td>\n      <td>0.000000</td>\n      <td>-5.640751e+01</td>\n      <td>-7.271573e+01</td>\n      <td>-4.832559e+01</td>\n      <td>-5.683171e+00</td>\n      <td>-1.137433e+02</td>\n      <td>-2.616051e+01</td>\n      <td>-4.355724e+01</td>\n      <td>-7.321672e+01</td>\n      <td>-1.343407e+01</td>\n      <td>...</td>\n      <td>-3.483038e+01</td>\n      <td>-1.093314e+01</td>\n      <td>-4.480774e+01</td>\n      <td>-2.836627e+00</td>\n      <td>-1.029540e+01</td>\n      <td>-2.604551e+00</td>\n      <td>-2.256568e+01</td>\n      <td>-1.543008e+01</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>25%</td>\n      <td>54201.500000</td>\n      <td>-9.203734e-01</td>\n      <td>-5.985499e-01</td>\n      <td>-8.903648e-01</td>\n      <td>-8.486401e-01</td>\n      <td>-6.915971e-01</td>\n      <td>-7.682956e-01</td>\n      <td>-5.540759e-01</td>\n      <td>-2.086297e-01</td>\n      <td>-6.430976e-01</td>\n      <td>...</td>\n      <td>-2.283949e-01</td>\n      <td>-5.423504e-01</td>\n      <td>-1.618463e-01</td>\n      <td>-3.545861e-01</td>\n      <td>-3.171451e-01</td>\n      <td>-3.269839e-01</td>\n      <td>-7.083953e-02</td>\n      <td>-5.295979e-02</td>\n      <td>5.600000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>50%</td>\n      <td>84692.000000</td>\n      <td>1.810880e-02</td>\n      <td>6.548556e-02</td>\n      <td>1.798463e-01</td>\n      <td>-1.984653e-02</td>\n      <td>-5.433583e-02</td>\n      <td>-2.741871e-01</td>\n      <td>4.010308e-02</td>\n      <td>2.235804e-02</td>\n      <td>-5.142873e-02</td>\n      <td>...</td>\n      <td>-2.945017e-02</td>\n      <td>6.781943e-03</td>\n      <td>-1.119293e-02</td>\n      <td>4.097606e-02</td>\n      <td>1.659350e-02</td>\n      <td>-5.213911e-02</td>\n      <td>1.342146e-03</td>\n      <td>1.124383e-02</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>75%</td>\n      <td>139320.500000</td>\n      <td>1.315642e+00</td>\n      <td>8.037239e-01</td>\n      <td>1.027196e+00</td>\n      <td>7.433413e-01</td>\n      <td>6.119264e-01</td>\n      <td>3.985649e-01</td>\n      <td>5.704361e-01</td>\n      <td>3.273459e-01</td>\n      <td>5.971390e-01</td>\n      <td>...</td>\n      <td>1.863772e-01</td>\n      <td>5.285536e-01</td>\n      <td>1.476421e-01</td>\n      <td>4.395266e-01</td>\n      <td>3.507156e-01</td>\n      <td>2.409522e-01</td>\n      <td>9.104512e-02</td>\n      <td>7.827995e-02</td>\n      <td>77.165000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>max</td>\n      <td>172792.000000</td>\n      <td>2.454930e+00</td>\n      <td>2.205773e+01</td>\n      <td>9.382558e+00</td>\n      <td>1.687534e+01</td>\n      <td>3.480167e+01</td>\n      <td>7.330163e+01</td>\n      <td>1.205895e+02</td>\n      <td>2.000721e+01</td>\n      <td>1.559499e+01</td>\n      <td>...</td>\n      <td>2.720284e+01</td>\n      <td>1.050309e+01</td>\n      <td>2.252841e+01</td>\n      <td>4.584549e+00</td>\n      <td>7.519589e+00</td>\n      <td>3.517346e+00</td>\n      <td>3.161220e+01</td>\n      <td>3.384781e+01</td>\n      <td>25691.160000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "#显示各个特征的一些统计学特征\n",
    "data_original.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据探索总结:  \n",
    "1. 数据中一共包含284807个样本  \n",
    "2. 数据一共包含30个特征列以及一个标签列  \n",
    "3. 数据的特征列和标签列下均没有缺失值，不需要进行缺失值处理  \n",
    "4. 数据的30个特征均为连续的数值特征(continuous numerical features)，没有类别特征(categorical features),不需要进行one-hot编码  \n",
    "5. 数据中两个类别（正常及欺诈）分布极不平衡，正常数据（非欺诈数据）所占比例为99.83%，欺诈数据所占比例为0.17%,因此这是一个非均衡数据集(imbalanced data)的分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. 数据集特征列和标签列的分离，训练集和测试集的分割  \n",
    ">为了尽可能避免数据信息泄露的问题，在对数据进行任何前处理之前，一定要先对数据进行训练集和测试集的分割  \n",
    ">[参考1：Normalize data before or after split of training and testing data?](https://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data)  \n",
    ">[参考2：Onehotencoding before or after split of training and testing data?](https://stackoverflow.com/questions/55525195/do-i-have-to-do-one-hot-encoding-separately-for-train-and-test-dataset)  \n",
    ">[参考3：Imputation before or after train test spliting](https://stats.stackexchange.com/questions/95083/imputation-before-or-after-splitting-into-train-and-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#首先对数据集进行特征，标签的分离\n",
    "X = data_original.iloc[:,0:-1]\n",
    "y = data_original['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...       V20       V21       V22       V23       V24  \\\n0  0.098698  0.363787  ...  0.251412 -0.018307  0.277838 -0.110474  0.066928   \n1  0.085102 -0.255425  ... -0.069083 -0.225775 -0.638672  0.101288 -0.339846   \n2  0.247676 -1.514654  ...  0.524980  0.247998  0.771679  0.909412 -0.689281   \n3  0.377436 -1.387024  ... -0.208038 -0.108300  0.005274 -0.190321 -1.175575   \n4 -0.270533  0.817739  ...  0.408542 -0.009431  0.798278 -0.137458  0.141267   \n\n        V25       V26       V27       V28  Amount  \n0  0.128539 -0.189115  0.133558 -0.021053  149.62  \n1  0.167170  0.125895 -0.008983  0.014724    2.69  \n2 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n3  0.647376 -0.221929  0.062723  0.061458  123.50  \n4 -0.206010  0.502292  0.219422  0.215153   69.99  \n\n[5 rows x 30 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>0.251412</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.069083</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.524980</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.208038</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>0.408542</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 30 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    0\n1    0\n2    0\n3    0\n4    0\nName: Class, dtype: int64"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#然后进行训练集和测试集的分割,75%训练集，25%测试集\n",
    "#注意，这里我们要采用分层抽样(stratified sampling)的方法，以保证训练集和测试集中类别的比例和总体数据类别的比例基本一致\n",
    "#此外，如果我们提前获知数据中的某一个特征是关键特征，那么在进行分层抽样的时候，也可以该特征的比例作为参考，进行抽样\n",
    "#参考链接：https://medium.com/@411.codebrain/train-test-split-vs-stratifiedshufflesplit-374c3dbdcc36\n",
    "#参考链接：https://zhuanlan.zhihu.com/p/49991313\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    213236\n1       369\nName: Class, dtype: int64"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# 显示训练集中各个类别的数量，用于计算scale_pos_weight\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_pos_weight = sum(negative instances) / sum(positive instances)\n",
    "# 这个参数是XGBoost和LightGBM两个模型在对非均衡数据进行分类时用于控制类别平衡的最关键的参数\n",
    "# 实际应用时，也可以考虑使用按上述公式计算得到的值的平方根\n",
    "# 参考1：https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "# 参考2：https://stats.stackexchange.com/questions/243207/what-is-the-proper-usage-of-scale-pos-weight-in-xgboost-for-imbalanced-datasets\n",
    "scale_pos_weight_1 = 213236 / 369\n",
    "scale_pos_weight_2 = np.sqrt(scale_pos_weight_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "577.8753387533875"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "scale_pos_weight_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "24.039037808393818"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "scale_pos_weight_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. 对于非均衡数据集（imbalanced data）的处理  \n",
    ">对于非均衡数据的处理，有多种思路:  \n",
    ">* **重采样**，包括上采样（Oversampling,也叫过采样）和下采样（Undersampling,也叫欠采样），其基本思路就是将数据中两类的数量调整均衡一些，  \n",
    "让少的变多,让多的变少,从而使非均衡数据变得均衡.在重采样以后，再利用各种机器学习分类模型对数据进行分类。  \n",
    ">>- 两种重采样各自的实现方式均有很多种，实践中，我们利用[imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.under_sampling)库来完成各种重采样的实现  \n",
    ">* **利用imbalaced-learn中的分类器**, 这些分类器具有处理非均衡数据的内在机制,比如[BalancedRandomForestClassifier](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.ensemble.BalancedRandomForestClassifier.html#)和[RUSBoostClassifier](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.ensemble.RUSBoostClassifier.html)  \n",
    ">* **利用XGBoost和LightGBM**,这两种基于GBDT的强分类器，均可以设置参数'scale_pos_weight'来处理这种非均衡数据  \n",
    ">>- scale_pos_weight = number of negative samples / number of positive samples  \n",
    ">>- 对于二分类问题，正例(positive)用1表示，反例(negative)用0表示  \n",
    ">* 模型评估准则(Metrics)：对于非均衡数据，不能再使用accuracy作为评估准则，可以考虑使用f1_score或者专门针对非均衡问题的评估准则，比如  \n",
    "[geometric_mean_score](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.metrics.geometric_mean_score.html#imblearn.metrics.geometric_mean_score)  \n",
    "\n",
    ">* [参考1:Dealing With Class Imbalanced Datasets For Classification](https://towardsdatascience.com/dealing-with-class-imbalanced-datasets-for-classification-2cc6fad99fd9)    \n",
    ">* [参考2:机器学习之类别不平衡问题 (3) —— 采样方法](https://www.cnblogs.com/massquantity/p/9382710.html)  \n",
    ">* [参考3:机器学习中的非均衡问题(imbalanced data)和应对方法](https://zhuanlan.zhihu.com/p/38687978)  \n",
    ">* [参考4:机器学习：如何解决机器学习中数据不平衡问题](https://www.jianshu.com/p/be343414dd24)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.特征缩放(Feature Scaling)  \n",
    ">1. 特征缩放的意义?  \n",
    "*进行缩放后，多维特征将具有相近的尺度，这将帮助梯度下降算法更快地收敛*  \n",
    "*[参考1:Feature Scaling with scikit-learn](https://benalexkeen.com/feature-scaling-with-scikit-learn/)*  \n",
    "*[参考2:为什么 feature scaling 会使 gradient descent 的收敛更好?](https://www.zhihu.com/question/37129350)*  \n",
    "*[参考3:机器学习（一）- feature scaling](https://blog.csdn.net/mike112223/article/details/74923096)*  \n",
    "*[参考4:sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) *  \n",
    "*[参考5:特征缩放的几种方法](https://www.cnblogs.com/HuZihu/p/9761161.html) *  \n",
    ">1. *仅对连续数值特征(continuous numeric features)进行特征缩放，对离散类别特征(discrete categorical features)不做缩放*  \n",
    "*[参考1:Should you ever standardise binary variables?](https://stats.stackexchange.com/questions/59392/should-you-ever-standardise-binary-variables)*  \n",
    "*[参考2:Dummy variables, is it necessary to standardize them?\n",
    "](https://stackoverflow.com/questions/50557129/dummy-variables-is-necessary-to-standardize-them)*  \n",
    ">3. *最常用的三种特征缩放方法: 最大最小值归一化(min-max scaling)，标准化(standardization)以及稳健缩放(RobustScaler):*  \n",
    "   *- min-max scaling(normalization):*  \n",
    "   $ X_{minmax} = \\frac{X - X_{min}}{X_{max} - X_{min}} $  \n",
    "   *- standardization(Z-score):*  \n",
    "   $ X_{std} = \\frac{X - \\mu }{\\sigma } $  *($\\mu 是均值$, $\\sigma$是标准差)*  \n",
    "   *- robustscaler:*  \n",
    "   $ X_{rb} = \\frac{X - Q_1(x) }{Q_3(x) - Q_1(x)} $ *($Q_1$是第一四分位数， $Q_3$是第三四分位数)*  \n",
    "   [参考1:Feature Scaling with scikit-learn](https://benalexkeen.com/feature-scaling-with-scikit-learn/) \n",
    ">4. *其他的特征缩放的方法: PowerTransformer and QuantileTransformer*\n",
    ">5. *[其他参考1：Compare the effect of different scalers on data with outliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py) *  \n",
    "*[其他参考2：Which advantages does MinMax scaling have over a standard scaling using the mean and the standard deviation?](https://www.quora.com/Which-advantages-does-MinMax-scaling-have-over-a-standard-scaling-using-the-mean-and-the-standard-deviation) *  \n",
    "*[其他参考3：How to Identify Outliers in your Data？](https://machinelearningmastery.com/how-to-identify-outliers-in-your-data/) *  \n",
    "*[其他参考4：How to Make Your Machine Learning Models Robust to Outliers？](https://www.kdnuggets.com/2018/08/make-machine-learning-models-robust-outliers.html) *      \n",
    ">6. *__注意__: 需要先把数据拆分成训练集与验证集，在训练集上计算出需要的数值（如均值和标准值），对训练集数据做标准化/归一化处理（不要在整个数据集上做标准化/归一化处理，因为这样会将验证集的信息带入到训练集中，这是一个非常容易犯的错误），然后再用之前计算出的数据（如均值和标准值）对验证集数据做相同的标准化/归一化处理.*  \n",
    "*[参考1：Difference between preprocessing train and test set before and after splitting](https://stats.stackexchange.com/questions/267012/difference-between-preprocessing-train-and-test-set-before-and-after-splitting) *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于之前对三种特征缩放方法的探索，我们最终选择采用标准化(standardization)  \n",
    ">详情请参见文件: with_feature_scaling_without_feature_selection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化-Standardization(Z-score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_std = StandardScaler()  \n",
    "X_train_copy_2 = X_train.copy(deep=True)\n",
    "X_train_std = scaler_std.fit_transform(X_train_copy_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "type(X_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "pandas.core.series.Series"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_values = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "type(y_train_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. 特征选择(Feature Selection)  \n",
    ">* 特征选择的本质：剔除冗余特征或不相关特征，对原始特征进行降维  \n",
    ">* 特征选择的意义：提高模型预测的准确性，减少过拟合-提升模型的泛化能力，提升模型训练速度，  \n",
    ">* 特征选择在机器学习流程中的位置：[特征选择要放在特征缩放之后进行](https://stackoverflow.com/questions/46062679/right-order-of-doing-feature-selection-pca-and-normalization)  \n",
    ">* 特征选择有多种方法，基本可以归为三类：过滤(Filter)，包裹(Wrapper) 和嵌入(Embedded)  \n",
    ">>* Filter方法不依赖于特定的模型，对特征重要性的判断不需要先经过模型训练，因此该方法是模型独立的，常用的方法有:  \n",
    "Welch's t-Test, Fisher score, Chi-Squared test, Relief-based, CFS(Correlation-based feature selection),  \n",
    "FCBF(Fast correlation-based filter)等等。\n",
    ">>* Wrapper方法需要先指定一个机器学习模型，并进行训练，然后根据模型训练的结果来对特征的重要性进行判断。问题在于，究竟应该选择哪种模型并没有定论，基于不同种类的模型得到的特征重要性结果可能不一致，解决的方法：一是投票，二是[欲训练什么算法，就选择该算法进行评估](https://zhuanlan.zhihu.com/p/74198735)。Wrapper常用的方法有：Recursive Feature Elimination(RFE,递归式特征消除), Simulated Annealing(SA，模拟退火), 以及Genetic Algorithm(GA, 遗传算法)等。其中，[遗传算法最能保证获得全局最优特征子集](http://www.feat.engineering/genetic-algorithms.html),但这种方法运行非常耗时，效率低下，仅建议在运算资源充足，项目时间充足的情况下使用。  \n",
    ">>* Embedded方法，利用正则化思想，将部分特征属性的权重变成零，从而完成特征选择。 常见的正则化有L1的Lasso，L2的Ridge和混合的Elastic Net。\n",
    ">* [参考1:Overview of feature selection methods](https://towardsdatascience.com/overview-of-feature-selection-methods-a2d115c7a8f7)  \n",
    "[参考2:Machine Learning Explained: Regularization](http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/)  \n",
    "[参考3:机器学习（六）：特征选择方法—Filter,Wrapper,Embedded](https://zhuanlan.zhihu.com/p/120924870)  \n",
    "[参考4:基于 Jupyter 的特征工程手册：特征选择（四）](https://zhuanlan.zhihu.com/p/138758083)  \n",
    "[参考5：特征选择，经典三刀](https://zhuanlan.zhihu.com/p/24635014)  \n",
    "[参考6：特征工程笔记](https://www.jianshu.com/p/03284dd5e0bf)  \n",
    "\n",
    ">* 本项目使用一种[Wrapper方法:Boruta](https://github.com/scikit-learn-contrib/boruta_py)进行特征选择,使用BorutaPy需注意：  \n",
    ">>* BorutaPy在设计算法时，是基于RandomForest的，而对其他一些基于树模型的高级算法，比如XGBoost和LightGBM的兼容性则不好，实际应用中经常出现各种错误，具体请查阅官方github的相关issues；  \n",
    ">>* 本人在尝试将XGBoost和LightGBM带入BorutaPy进行特征选择时，也因兼容性问题，报错而无法继续；  \n",
    ">>* 目前，根据测试，BorutaPy与传统的RandomForest以及经过改良的专门针对非均衡数据的BalancedRandomForest具有良好的兼容性\n",
    ">* Boruta在进行特征选择的过程中，会试图找到包含对模型预测有用信息的所有特征  \n",
    ">* [其他有关Boruta的参考1：Select Important Variables using Boruta Algorithm](https://www.datasciencecentral.com/profiles/blogs/select-important-variables-using-boruta-algorithm)  \n",
    ">* [其他有关Boruta的参考2：Boruta explained exactly how you wished someone explained to you](https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a)  \n",
    ">* [其他有关Boruta的参考3：Feature selection? You are probably doing it wrong](https://towardsdatascience.com/feature-selection-you-are-probably-doing-it-wrong-985679b41456)  \n",
    ">* 目前已有基于Boruta的方法，但采用XGBoost模型进行特征选择的库，比如：[BoostARoota](https://github.com/chasedehan/BoostARoota)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载相关库\n",
    "from boruta import BorutaPy\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对于非均衡数据的class_weight的计算方法\n",
    "#参考1：https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53696\n",
    "#参考2：https://stackoverflow.com/questions/44716150/how-can-i-assign-a-class-weight-in-keras-in-a-simple-way\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',classes = np.unique(y_train),y = y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "type(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "type(class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([  0.50086524, 289.43766938])"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{0: 0.5008652385150725, 1: 289.43766937669375}"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9',\n       'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n       'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27',\n       'V28', 'Amount'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# 返回数据集的特征名\n",
    "# 参考链接：https://stackoverflow.com/questions/19482970/get-list-from-pandas-dataframe-column-headers\n",
    "Columns = X_train.columns.to_numpy()\n",
    "Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义BalancedRandomForestClassifier，使用所有CPU核进行计算\n",
    "#注意class_weight的计算方法\n",
    "#注意，BorutaPy作者建议将max_depth设置为3-7\n",
    "#此处构建模型只需要设定一些基本值即可，不涉及超参数调节的问题\n",
    "brf = BalancedRandomForestClassifier(n_jobs=-1, class_weight = class_weight_dict, max_depth=7,random_state=42)\n",
    "feat_selector_brf = BorutaPy(brf, n_estimators='auto', verbose=2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**1. 使用BalancedRandomForestClassifier进行特征选择**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iteration: \t1 / 100\nConfirmed: \t0\nTentative: \t30\nRejected: \t0\nIteration: \t2 / 100\nConfirmed: \t0\nTentative: \t30\nRejected: \t0\nIteration: \t3 / 100\nConfirmed: \t0\nTentative: \t30\nRejected: \t0\nIteration: \t4 / 100\nConfirmed: \t0\nTentative: \t30\nRejected: \t0\nIteration: \t5 / 100\nConfirmed: \t0\nTentative: \t30\nRejected: \t0\nIteration: \t6 / 100\nConfirmed: \t0\nTentative: \t30\nRejected: \t0\nIteration: \t7 / 100\nConfirmed: \t0\nTentative: \t30\nRejected: \t0\nIteration: \t8 / 100\nConfirmed: \t6\nTentative: \t3\nRejected: \t21\nIteration: \t9 / 100\nConfirmed: \t6\nTentative: \t3\nRejected: \t21\nIteration: \t10 / 100\nConfirmed: \t6\nTentative: \t3\nRejected: \t21\nIteration: \t11 / 100\nConfirmed: \t6\nTentative: \t3\nRejected: \t21\nIteration: \t12 / 100\nConfirmed: \t6\nTentative: \t3\nRejected: \t21\nIteration: \t13 / 100\nConfirmed: \t6\nTentative: \t3\nRejected: \t21\nIteration: \t14 / 100\nConfirmed: \t6\nTentative: \t3\nRejected: \t21\nIteration: \t15 / 100\nConfirmed: \t6\nTentative: \t3\nRejected: \t21\nIteration: \t16 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t17 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t18 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t19 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t20 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t21 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t22 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t23 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t24 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t25 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t26 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t27 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t28 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t29 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t30 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t31 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t32 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t33 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t34 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t35 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t36 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t37 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t38 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t39 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t40 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t41 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t42 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t43 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t44 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t45 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t46 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t47 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t48 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t49 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t50 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t51 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t52 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t53 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t54 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t55 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t56 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t57 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t58 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t59 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t60 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t61 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t62 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t63 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t64 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t65 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t66 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t67 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t68 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t69 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t70 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t71 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t72 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t73 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t74 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t75 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t76 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t77 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t78 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t79 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t80 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t81 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t82 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t83 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t84 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t85 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t86 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t87 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t88 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t89 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t90 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t91 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t92 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t93 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t94 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t95 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t96 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t97 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t98 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\nIteration: \t99 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\n\n\nBorutaPy finished running.\n\nIteration: \t100 / 100\nConfirmed: \t7\nTentative: \t2\nRejected: \t21\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BorutaPy(alpha=0.05,\n         estimator=BalancedRandomForestClassifier(bootstrap=True,\n                                                  class_weight={0: 0.5008652385150725,\n                                                                1: 289.43766937669375},\n                                                  criterion='gini', max_depth=7,\n                                                  max_features='auto',\n                                                  max_leaf_nodes=None,\n                                                  min_impurity_decrease=0.0,\n                                                  min_samples_leaf=2,\n                                                  min_samples_split=2,\n                                                  min_weight_fraction_leaf=0.0,\n                                                  n_estimators=60, n_jobs=-1,\n                                                  oob_score=False,\n                                                  random_state=<mtrand.RandomState object at 0x000002B0E3F344A8>,\n                                                  replacement=False,\n                                                  sampling_strategy='auto',\n                                                  verbose=0, warm_start=False),\n         max_iter=100, n_estimators='auto', perc=100,\n         random_state=<mtrand.RandomState object at 0x000002B0E3F344A8>,\n         two_step=True, verbose=2)"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "#执行特征选择\n",
    "feat_selector_brf.fit(X_train_std,y_train_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boruta进行特征选择过程后，结果包含三类.第一类，确定对模型预测有价值的特征，即确定要保留的特征；第二类，确定对模型预测无价值的特征，即确定被剔除的特征；第三类，#Boruta无法确定是否对模型预测有价值，即无法确定是否这些特征应该被保留还是剔除，是否保留由我们决定。其中support_中包含着第一类，而support_weak_中包含着第三类。\n",
    "confirmed_brf = feat_selector_brf.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "type(confirmed_brf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([False,  True, False,  True,  True, False, False, False, False,\n       False, False,  True,  True, False,  True, False, False, False,\n       False, False, False, False, False, False,  True, False, False,\n       False, False, False])"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "confirmed_brf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['V1', 'V3', 'V4', 'V11', 'V12', 'V14', 'V24'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "#返回确定要保留的特征名\n",
    "features_confirmed_brf = Columns[confirmed_brf]\n",
    "features_confirmed_brf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#返回BorutaPy无法决定是否保留或剔除的特征，即存疑特征\n",
    "tentative_brf = feat_selector_brf.support_weak_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ True, False, False, False, False, False, False, False, False,\n        True, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False])"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "tentative_brf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['Time', 'V9'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "#返回存疑特征名,这些特征是否保留由我们自己决定。\n",
    "features_tentative_brf = Columns[tentative_brf]\n",
    "features_tentative_brf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 2,  1, 14,  1,  1, 11,  9,  9,  4,  2,  3,  1,  1,  6,  1, 17,  7,\n       12,  7, 22, 19, 15, 16, 23,  1, 19,  5, 13, 21, 19])"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "#值为1的特征最重要，为Boruta确定要保留的，值为2的特征为存疑特征，是否保留由我们自己决定，其他特征为被剔除的特征\n",
    "rank_brf = feat_selector_brf.ranking_\n",
    "rank_brf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['V14', 'V1', 'V24', 'V3', 'V4', 'V12', 'V11', 'V9', 'Time', 'V10',\n       'V8', 'V26', 'V13', 'V16', 'V18', 'V7', 'V6', 'V5', 'V17', 'V27',\n       'V2', 'V21', 'V22', 'V15', 'V25', 'Amount', 'V20', 'V28', 'V19',\n       'V23'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "#参考链接：https://www.codenong.com/6618515/\n",
    "features_brf_sorted = Columns[rank_brf.argsort()]\n",
    "features_brf_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#返回确定保留的特征名\n",
    "features_filtered_brf_narrow = features_confirmed_brf\n",
    "#返回确定保留的特征加存疑特征名\n",
    "features_filtered_brf_broad = np.unique(np.concatenate((features_confirmed_brf,features_tentative_brf),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['V1', 'V3', 'V4', 'V11', 'V12', 'V14', 'V24'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "features_filtered_brf_narrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['Time', 'V1', 'V11', 'V12', 'V14', 'V24', 'V3', 'V4', 'V9'],\n      dtype=object)"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "features_filtered_brf_broad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_filtered_narrow只保留了确定特征，而features_filtered_broad保留了确定和存疑特征\n",
    "X_train_filtered_brf_narrow = feat_selector_brf.transform(X_train_std)\n",
    "#weak = True,包含存疑特征\n",
    "X_train_filtered_brf_broad = feat_selector_brf.transform(X_train_std,weak = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(213605, 7)"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "X_train_filtered_brf_narrow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(213605, 9)"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "X_train_filtered_brf_broad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. 训练模型并使用[Hyperopt](http://hyperopt.github.io/hyperopt/)进行超参数的调优  \n",
    ">* hyperopt是一种通过**贝叶斯优化(Bayesian Optimization)**来调整参数的工具  \n",
    ">* 三种调参方法GridSearch,RandomSearch以及Bayesian Search的对比可参见：  \n",
    ">>* [Intuitive Hyperparameter Optimization : Grid Search, Random Search and Bayesian Search](https://towardsdatascience.com/intuitive-hyperparameter-optimization-grid-search-random-search-and-bayesian-search-2102dbfaf5b)  \n",
    ">>* [贝叶斯优化: 一种更好的超参数调优方式](https://zhuanlan.zhihu.com/p/29779000)  \n",
    ">* 本项目采用十折交叉验证法进行参数调优,模型评估准则采用专门针对非均衡数据的准则geometric_mean_score  \n",
    ">* 思路：  \n",
    ">>1. 首先对未进行特征缩放的数据进行训练和测试，查看结果  \n",
    ">>2. 然后对经过特征缩放但未经过特征选择的数据进行训练和测试，查看结果  \n",
    ">>3. 最后对经过特征缩放和特征选择的数据进行训练和测试，查看结果  \n",
    ">* [RandomForest调参参考](https://www.cnblogs.com/pinard/p/6160412.html)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from sklearn.metrics import make_scorer\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from hyperopt import fmin, tpe, atpe, hp, STATUS_OK, Trials, space_eval # (atpe) adaptive TPE 算法是hyperopt最新版本加入的新算法\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对于非均衡数据的class_weight的计算方法\n",
    "#参考1：https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53696\n",
    "#参考2：https://stackoverflow.com/questions/44716150/how-can-i-assign-a-class-weight-in-keras-in-a-simple-way\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',classes = np.unique(y_train),y = y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "type(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([  0.50086524, 289.43766938])"
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict"
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "type(class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{0: 0.5008652385150725, 1: 289.43766937669375}"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100%|██████████| 30/30 [27:41<00:00, 55.39s/trial, best loss: -0.9403892094531207]\n{'class_weight': {0: 0.5008652385150725, 1: 289.43766937669375}, 'max_depth': 60, 'max_features': 1, 'n_estimators': 120, 'n_jobs': -1, 'random_state': 42, 'warm_start': True}\n-0.9403892094531207\nTraining time: 1661.680s\n"
    }
   ],
   "source": [
    "#BalancedRandomForest分类器的训练\n",
    "start = time.time()\n",
    "def brdf(params):\n",
    "    # ip = params[\"imputer\"]\n",
    "    # del params[\"imputer\"]\n",
    "    # sc = params[\"scaler\"]\n",
    "    # del params[\"scaler\"]\n",
    "    brdf_clf = BalancedRandomForestClassifier(**params)\n",
    "    str_kfold = StratifiedKFold(\n",
    "        n_splits=10, shuffle=True, random_state=42\n",
    "    )  # 注意随机数random_state保持一致，以便复现结果\n",
    "    # 参考链接：https://stackoverflow.com/questions/39782243/how-to-use-cross-val-score-with-random-state\n",
    "    gms = make_scorer(geometric_mean_score)\n",
    "    metric = cross_val_score(\n",
    "        brdf_clf,\n",
    "        # Data_to_opt(sc)[0],\n",
    "        # Data_to_opt(sc)[1],\n",
    "        X_train_filtered_brf_broad,\n",
    "        y_train,\n",
    "        cv=str_kfold,\n",
    "        scoring = gms, \n",
    "        n_jobs=-1,  \n",
    "    ).mean()  \n",
    "    return {\"loss\": -metric, \"status\": STATUS_OK}\n",
    "\n",
    "space4brdf = {\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", range(100, 320, 20)),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", range(1, 70)),  #! max_depth 影响模型的复杂程度\n",
    "    # \"max_features\": 1,\n",
    "    \"max_features\": hp.choice(\"max_features\", range(1, 9)),\n",
    "    \"class_weight\": class_weight_dict, # 这是BalancedRandomForestClassifier用于控制各类平衡的关键参数\n",
    "    \"warm_start\": True,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,  # 注意保持随机状态的一致性，以便复现结果\n",
    "    #\"imputation\": hp.choice(\"imputation\", [\"dropna\", \"SI\", \"MI\"]),\n",
    "    #\"scaling_method\": hp.choice(\"scaling_method\", [\"min_max\", \"std\"]),\n",
    "}\n",
    "\n",
    "rstate = np.random.RandomState(42)\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    brdf, space4brdf, algo=tpe.suggest, max_evals=30, trials=trials, rstate=rstate\n",
    ")  #! fmin返回的是这些最佳参数在其列表中的索引，而不是直接返回最佳参数本身\n",
    "# print(best)\n",
    "print(space_eval(space4brdf, best))  #! space_eval()输出最佳参数本身而不是索引\n",
    "# print(lgt(best))\n",
    "print(trials.best_trial[\"result\"][\"loss\"])\n",
    "# print(trials.best_trial[\"result\"])\n",
    "\n",
    "# 把最终搜索到的最有超参数写入到一个json文件\n",
    "# 参考链接: https://stackabuse.com/scikit-learn-save-and-restore-models/\n",
    "with open(\"brdf.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\"f1\": trials.best_trial[\"result\"][\"loss\"], \"Best params\": space_eval(space4brdf, best)}))\n",
    "hyperparams_brdf = space_eval(space4brdf, best)\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100%|██████████| 30/30 [36:31<00:00, 73.06s/trial, best loss: -0.9446299227077104]\n{'learning_rate': 0.2562966159166658, 'n_estimators': 70, 'random_state': 42}\n-0.9446299227077104\nTraining time: 2191.754s\n"
    }
   ],
   "source": [
    "#RUSBoostClassifier的训练及参数调优\n",
    "start = time.time()\n",
    "def rusb(params):\n",
    "    # ip = params[\"imputer\"]\n",
    "    # del params[\"imputer\"]\n",
    "    # sc = params[\"scaler\"]\n",
    "    # del params[\"scaler\"]\n",
    "    rusb_clf = RUSBoostClassifier(**params)\n",
    "    str_kfold = StratifiedKFold(\n",
    "        n_splits=10, shuffle=True, random_state=42\n",
    "    )  # 注意随机数random_state保持一致，以便复现结果\n",
    "    # 参考链接：https://stackoverflow.com/questions/39782243/how-to-use-cross-val-score-with-random-state\n",
    "    gms = make_scorer(geometric_mean_score)\n",
    "    metric = cross_val_score(\n",
    "        rusb_clf,\n",
    "        # Data_to_opt(sc)[0],\n",
    "        # Data_to_opt(sc)[1],\n",
    "        X_train_filtered_brf_broad,\n",
    "        y_train,\n",
    "        cv=str_kfold,\n",
    "        scoring = gms, \n",
    "        n_jobs=-1,  \n",
    "    ).mean()  \n",
    "    return {\"loss\": -metric, \"status\": STATUS_OK}\n",
    "\n",
    "space4rusb = {\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", range(50, 320, 20)),\n",
    "    # \"max_depth\": hp.choice(\"max_depth\", range(1, 70)),  \n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0, 1),\n",
    "    # \"max_features\": 1,\n",
    "    # \"max_features\": hp.choice(\"max_features\", range(1, 30)),\n",
    "    # \"class_weight\": class_weight_dict,\n",
    "    # \"warm_start\": True,\n",
    "    # \"n_jobs\": -1,\n",
    "    \"random_state\": 42,  \n",
    "    #\"imputation\": hp.choice(\"imputation\", [\"dropna\", \"SI\", \"MI\"]),\n",
    "    #\"scaling_method\": hp.choice(\"scaling_method\", [\"min_max\", \"std\"]),\n",
    "}\n",
    "\n",
    "rstate = np.random.RandomState(42)\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    rusb, space4rusb, algo=tpe.suggest, max_evals=30, trials=trials, rstate=rstate\n",
    ")  #! fmin返回的是这些最佳参数在其列表中的索引，而不是直接返回最佳参数本身\n",
    "# print(best)\n",
    "print(space_eval(space4rusb, best))  #! space_eval()输出最佳参数本身而不是索引\n",
    "# print(lgt(best))\n",
    "print(trials.best_trial[\"result\"][\"loss\"])\n",
    "# print(trials.best_trial[\"result\"])\n",
    "\n",
    "with open(\"rusb.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\"f1\": trials.best_trial[\"result\"][\"loss\"], \"Best params\": space_eval(space4rusb, best)}))\n",
    "hyperparams_rusb = space_eval(space4rusb, best)\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100%|██████████| 30/30 [16:12<00:00, 32.41s/trial, best loss: -0.9318785441163249]\n{'colsample_bytree': 0.39779512373111203, 'gamma': 5, 'learning_rate': 0.326749198175752, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 50, 'n_jobs': -1, 'objective': 'binary:logistic', 'random_state': 42, 'scale_pos_weight': 577.8753387533875, 'subsample': 0.625032357077646, 'tree_method': 'hist'}\n-0.9318785441163249\nTraining time: 972.361s\n"
    }
   ],
   "source": [
    "#XGBoostClassifier的训练和参数调优\n",
    "start = time.time()\n",
    "def xgb(params):\n",
    "    # ip = params[\"imputer\"]\n",
    "    # del params[\"imputer\"]\n",
    "    # sc = params[\"scaler\"]\n",
    "    # del params[\"scaler\"]\n",
    "    xgb_clf = XGBClassifier(**params)\n",
    "    str_kfold = StratifiedKFold(\n",
    "        n_splits=10, shuffle=True, random_state=42\n",
    "    )  #!Here the random state should be the same as that in the model\n",
    "    # ?https://stackoverflow.com/questions/39782243/how-to-use-cross-val-score-with-random-state\n",
    "    gms = make_scorer(geometric_mean_score)\n",
    "    metric = cross_val_score(\n",
    "        xgb_clf,\n",
    "        # Data_to_opt(sc)[0],\n",
    "        # Data_to_opt(sc)[1],\n",
    "        X_train_filtered_brf_broad,\n",
    "        y_train,\n",
    "        cv=str_kfold,\n",
    "        scoring = gms, \n",
    "        n_jobs=-1,  \n",
    "    ).mean()  \n",
    "    return {\"loss\": -metric, \"status\": STATUS_OK}\n",
    "\n",
    "space4xgb = {\n",
    "    \"max_depth\": hp.choice(\"max_depth\", range(3, 20)),  \n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0, 1),\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", [50, 100, 150, 200, 250, 300]),\n",
    "    # \"objective\": \"multi:softmax\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"scale_pos_weight\": hp.choice(\"scale_pos_weight\", [scale_pos_weight_1,scale_pos_weight_2]),\n",
    "    \"n_jobs\": -1,\n",
    "    \"gamma\": hp.randint(\"gamma\", 10),\n",
    "    \"min_child_weight\": hp.choice(\"min_child_weight\", range(1, 10)),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.1, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.1, 1.0),\n",
    "    \"random_state\": 42,  \n",
    "    \"tree_method\": \"hist\",\n",
    "    # \"imputation\": hp.choice(\"imputation\", [\"dropna\", \"SI\", \"MI\"]),\n",
    "    # \"scaling_method\": hp.choice(\"scaling_method\", [\"min_max\", \"std\"]),\n",
    "}\n",
    "\n",
    "rstate = np.random.RandomState(42)\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    xgb, space4xgb, algo=tpe.suggest, max_evals=30, trials=trials, rstate=rstate\n",
    ")  #! fmin返回的是这些最佳参数在其列表中的索引，而不是直接返回最佳参数本身\n",
    "# print(best)\n",
    "print(space_eval(space4xgb, best))  #! space_eval()输出最佳参数本身而不是索引\n",
    "# print(lgt(best))\n",
    "print(trials.best_trial[\"result\"][\"loss\"])\n",
    "# print(trials.best_trial[\"result\"])\n",
    "\n",
    "\n",
    "with open(\"xgb.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\"f1\": trials.best_trial[\"result\"][\"loss\"], \"Best params\": space_eval(space4xgb, best)}))\n",
    "hyperparams_xgb =  space_eval(space4xgb, best)\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100%|██████████| 30/30 [10:05<00:00, 20.19s/trial, best loss: -0.9216389649107372]\n{'colsample_bytree': 0.6637068249287105, 'device': 'gpu', 'gpu_device_id': 0, 'gpu_platform_id': 0, 'gpu_use_dp': False, 'learning_rate': 0.023186776303918973, 'max_bin': 63, 'max_depth': 4, 'min_child_samples': 5, 'min_child_weight': 9.995289243479617, 'min_split_gain': 19.079058859531695, 'n_jobs': -1, 'num_boost_round': 125, 'num_leaves': 317, 'objective': 'binary', 'random_state': 42, 'scale_pos_weight': 577.8753387533875, 'subsample': 0.30670170310598655, 'subsample_freq': 29}\n-0.9216389649107372\nTraining time: 606.540s\n"
    }
   ],
   "source": [
    "#LightGBMClassifier的训练和恶参数调优\n",
    "start = time.time()\n",
    "def lgbm(params):\n",
    "    # ip = params[\"imputer\"]\n",
    "    # del params[\"imputer\"]\n",
    "    # sc = params[\"scaler\"]\n",
    "    # del params[\"scaler\"]\n",
    "    lgbm_clf = LGBMClassifier(**params)\n",
    "    str_kfold = StratifiedKFold(\n",
    "        n_splits=10, shuffle=True, random_state=42\n",
    "    )  \n",
    "    gms = make_scorer(geometric_mean_score)\n",
    "    metric = cross_val_score(\n",
    "        lgbm_clf,\n",
    "        # Data_to_opt(sc)[0],\n",
    "        # Data_to_opt(sc)[1],\n",
    "        X_train_filtered_brf_broad,\n",
    "        y_train,\n",
    "        cv=str_kfold,\n",
    "        scoring = gms, \n",
    "        n_jobs=-1,  \n",
    "    ).mean()  \n",
    "    return {\"loss\": -metric, \"status\": STATUS_OK}\n",
    "\n",
    "space4lgbm = {\n",
    "    # 参考链接： https://lightgbm.readthedocs.io/en/latest/Parameters.html#max_bin\n",
    "    # max_bin: int, default =255, >1,\n",
    "    # smaller max_bin, faster speed, maybe underfitting; larger max_bin, slower speed, maybe overfitting\n",
    "    \"max_bin\": 63,\n",
    "    \"num_leaves\": hp.choice(\"num_leaves\", range(100, 500)),  # * the larger this value, the more complex the model is\n",
    "    \"max_depth\": hp.choice(\"max_depth\", range(3, 32)),\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.2),\n",
    "    # \"n_estimators\": hp.choice(\"n_estimators\", [50, 100, 150, 200, 250, 300]), #! n_estimators has bug here\n",
    "    \"num_boost_round\": hp.choice(\"num_boost_round\", range(50, 500)),  #! this is an alias of n_estimators but no bug\n",
    "    # \"objective\": \"multiclass\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"n_jobs\": -1,\n",
    "    # \"class_weight\": \"balanced\",  #! This set should be done when the classes are imbalanced\n",
    "    \"scale_pos_weight\": hp.choice(\"scale_pos_weight\", [scale_pos_weight_1,scale_pos_weight_2]), #控制非均衡是数据中各个类别的平衡\n",
    "    \"min_split_gain\": hp.uniform(\"gamma\", 0, 50),  #! this is the 'gamma' in xgboost but its type is float now\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0, 10),\n",
    "    \"min_child_samples\": hp.randint(\"min_child_samples\", 20),  #! too large may cause underfitting\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.1, 1.0),\n",
    "    \"subsample_freq\": hp.choice(\"subsample_freq\", range(1, 30)),  #! k means perform bagging at every k iteration\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.1, 1.0),\n",
    "    \"random_state\": 42,  #! Here the random state should be the same as that in the stratify Kfold setttings\n",
    "    \"gpu_use_dp\": False,  #! for result's reproducibility\n",
    "    \"device\": \"gpu\",\n",
    "    \"gpu_platform_id\": 0,  # *OpenCL platform ID   小规模的CPU会快，大规模的GPU会快\n",
    "    \"gpu_device_id\": 0,  # *OpenCL device ID\n",
    "    # \"imputation\": hp.choice(\"imputation\", [\"dropna\", \"SI\", \"MI\"]),\n",
    "    # \"scaling_method\": hp.choice(\"scaling_method\", [\"min_max\", \"std\"]),\n",
    "}#!fmin needs this random state for reproducibility, and all the random seed should be the same as above.\n",
    "rstate = np.random.RandomState(42)\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    lgbm, space4lgbm, algo=tpe.suggest, max_evals=30, trials=trials, rstate=rstate\n",
    ")  #! fmin返回的是这些最佳参数在其列表中的索引，而不是直接返回最佳参数本身\n",
    "# print(best)\n",
    "print(space_eval(space4lgbm, best))  #! space_eval()输出最佳参数本身而不是索引\n",
    "# print(lgt(best))\n",
    "print(trials.best_trial[\"result\"][\"loss\"])\n",
    "# print(trials.best_trial[\"result\"])\n",
    "\n",
    "\n",
    "with open(\"lgbm.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\"f1\": trials.best_trial[\"result\"][\"loss\"], \"Best params\": space_eval(space4lgbm, best)}))\n",
    "hyperparams_lgbm = space_eval(space4lgbm, best)\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII.测试，检验模型的泛化能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling on Test Set  \n",
    "> *Fit on training set and transform on test set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std\n",
    "X_train_copy_5 = X_train.copy(deep=True)\n",
    "scaler_std.fit(X_train_copy_5)\n",
    "X_test_copy_2 = X_test.copy(deep=True)\n",
    "X_test_std = scaler_std.transform(X_test_copy_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection on Test Set  \n",
    "> *Fit on training set and transform on test set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_filtered_narrow只保留了确定特征，而features_filtered_broad保留了确定和存疑特征\n",
    "X_test_filtered_brf_narrow = feat_selector_brf.transform(X_test_std)\n",
    "#weak = True,包含存疑特征\n",
    "X_test_filtered_brf_broad = feat_selector_brf.transform(X_test_std,weak = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BalancedRandomForestClassifier(bootstrap=True,\n                               class_weight={0: 0.5008652385150725,\n                                             1: 289.43766937669375},\n                               criterion='gini', max_depth=60, max_features=1,\n                               max_leaf_nodes=None, min_impurity_decrease=0.0,\n                               min_samples_leaf=2, min_samples_split=2,\n                               min_weight_fraction_leaf=0.0, n_estimators=120,\n                               n_jobs=-1, oob_score=False, random_state=42,\n                               replacement=False, sampling_strategy='auto',\n                               verbose=0, warm_start=True)"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "# BalancedRandomForestClassifier Refit\n",
    "brdf_refit = BalancedRandomForestClassifier(**hyperparams_brdf)\n",
    "brdf_refit.fit(X_train_filtered_brf_broad, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RUSBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n                   learning_rate=0.2562966159166658, n_estimators=70,\n                   random_state=42, replacement=False,\n                   sampling_strategy='auto')"
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "# RUSBoostClassifier Refit\n",
    "rusb_refit = RUSBoostClassifier(**hyperparams_rusb)\n",
    "rusb_refit.fit(X_train_filtered_brf_broad, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.39779512373111203, gamma=5,\n              learning_rate=0.326749198175752, max_delta_step=0, max_depth=3,\n              min_child_weight=2, missing=None, n_estimators=50, n_jobs=-1,\n              nthread=None, objective='binary:logistic', random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=577.8753387533875,\n              seed=None, silent=None, subsample=0.625032357077646,\n              tree_method='hist', verbosity=1)"
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "# XGBoostClassifier Refit\n",
    "xgb_refit = XGBClassifier(**hyperparams_xgb)\n",
    "xgb_refit.fit(X_train_filtered_brf_broad, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LGBMClassifier(boosting_type='gbdt', class_weight=None,\n               colsample_bytree=0.6637068249287105, device='gpu',\n               gpu_device_id=0, gpu_platform_id=0, gpu_use_dp=False,\n               importance_type='split', learning_rate=0.023186776303918973,\n               max_bin=63, max_depth=4, min_child_samples=5,\n               min_child_weight=9.995289243479617,\n               min_split_gain=19.079058859531695, n_estimators=100, n_jobs=-1,\n               num_boost_round=125, num_leaves=317, objective='binary',\n               random_state=42, reg_alpha=0.0, reg_lambda=0.0,\n               scale_pos_weight=577.8753387533875, silent=True,\n               subsample=0.30670170310598655, subsample_for_bin=200000,\n               subsample_freq=29)"
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "# LGBMClassifier Refit\n",
    "lgbm_refit = LGBMClassifier(**hyperparams_lgbm)\n",
    "lgbm_refit.fit(X_train_filtered_brf_broad, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Final geometric_mean_score: 0.9336766629451787\n"
    }
   ],
   "source": [
    "# BalancedRandomForestClassifier Test\n",
    "y_test_pred = brdf_refit.predict(X_test_filtered_brf_broad)\n",
    "gms = geometric_mean_score(y_test, y_test_pred, average=\"binary\")  \n",
    "print(\"Final geometric_mean_score:\", gms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Final geometric_mean_score: 0.9355062114318377\n"
    }
   ],
   "source": [
    "# RUSBoostClassifier Test\n",
    "y_test_pred = rusb_refit.predict(X_test_filtered_brf_broad)\n",
    "gms = geometric_mean_score(y_test, y_test_pred, average=\"binary\")  \n",
    "print(\"Final geometric_mean_score:\", gms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Final geometric_mean_score: 0.9411245957300625\n"
    }
   ],
   "source": [
    "# XGBoostClassifier Test\n",
    "y_test_pred = xgb_refit.predict(X_test_filtered_brf_broad)\n",
    "gms = geometric_mean_score(y_test, y_test_pred, average=\"binary\")  \n",
    "print(\"Final geometric_mean_score:\", gms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Final geometric_mean_score: 0.9202913971283998\n"
    }
   ],
   "source": [
    "# LGBMClassifier Test\n",
    "y_test_pred = lgbm_refit.predict(X_test_filtered_brf_broad)\n",
    "gms = geometric_mean_score(y_test, y_test_pred, average=\"binary\")  \n",
    "print(\"Final geometric_mean_score:\", gms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIII.结果汇总：  \n",
    "![运行结果](./results_std_fs.png)  \n",
    "![运行结果](./barchart.png)  \n",
    "![运行结果](./barchart_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX.结果分析  \n",
    ">1. 经特征选择，四种模型的训练速度均得到了一定程度的提升  \n",
    ">2. 对于LightGBM,特征选择对其预测能力略有降低  \n",
    ">3. 对于其他三种模型，特征选择其预测能力均有一定程度的提升  \n",
    ">4. BorutaPy分析得到的两个存疑特征：'Time' 和 'V9', 对于除LightGBM之外的其他三种模型均是关键特征  \n",
    ">5. 本项目中，最终对信用卡数据进行欺诈检测表现最优的组合是：  \n",
    ">>* 模型：XGBoostClassifier  \n",
    ">>* 特征缩放：standardization（标准化）  \n",
    ">>* 特征：\\['Time', 'V1', 'V11', 'V12', 'V14', 'V24', 'V3', 'V4', 'V9'\\]  \n",
    ">>* 最终得分(geometric_mean_score): 0.9411  \n",
    ">>* 训练时间(秒)：972.361s"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitb5a08d408f8d409489ba603a7485d697",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}