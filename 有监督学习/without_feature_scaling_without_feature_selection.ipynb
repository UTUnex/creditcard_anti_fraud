{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信用卡反欺诈检测之基于imbalanced-learn,XGBoost和LightGBM的有监督学习实现  \n",
    ">1. 数据及项目来源：[Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud)  \n",
    ">2. 问题类别：**有监督学习的二分类问题**或者是**无监督学习的异常检测问题**    \n",
    ">3. 有监督学习方案：使用imbalanced-learn中的BalancedRandomForestClassifier,RUSBoostClassifier以及XGBoost和LightGBM四种模型对数据进行分类  \n",
    ">4. 无监督学习方案：使用Isolation Forest（孤立森林）对数据进行异常检测  \n",
    "\n",
    ">5. 思路：单一变量原则，逐渐叠加影响因子  \n",
    ">>1. 首先对未进行特征缩放的数据进行训练和测试，查看结果，文件：without_feature_scaling_without_feature_selection.ipynb  \n",
    ">>2. 然后对经过特征缩放但未经过特征选择的数据进行训练和测试，查看结果  文件：with_feature_scaling_without_feature_selection.ipynb  \n",
    ">>3. 最后对经过特征缩放和特征选择的数据进行训练和测试，查看结果  文件：with_feature_scaling_with_feature_selection.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. 加载数据并对其进行初步的探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据前处理的通用库numpy和pandas\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据为Pandas dataframe格式\n",
    "data_original = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "pandas.core.frame.DataFrame"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#数据为Pandas dataframe格式\n",
    "type(data_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n5  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427 -0.232794   \n6  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104 -0.780055  0.750137   \n7 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709 -0.415267   \n8  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233  1.011592  0.373205   \n9  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794 -0.385050 -0.069733   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n5  0.105915  0.253844  0.081080    3.67      0  \n6 -0.257237  0.034507  0.005168    4.99      0  \n7 -0.051634 -1.206921 -1.085339   40.80      0  \n8 -0.384157  0.011747  0.142404   93.20      0  \n9  0.094199  0.246219  0.083076    3.68      0  \n\n[10 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.0</td>\n      <td>-0.425966</td>\n      <td>0.960523</td>\n      <td>1.141109</td>\n      <td>-0.168252</td>\n      <td>0.420987</td>\n      <td>-0.029728</td>\n      <td>0.476201</td>\n      <td>0.260314</td>\n      <td>-0.568671</td>\n      <td>...</td>\n      <td>-0.208254</td>\n      <td>-0.559825</td>\n      <td>-0.026398</td>\n      <td>-0.371427</td>\n      <td>-0.232794</td>\n      <td>0.105915</td>\n      <td>0.253844</td>\n      <td>0.081080</td>\n      <td>3.67</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>4.0</td>\n      <td>1.229658</td>\n      <td>0.141004</td>\n      <td>0.045371</td>\n      <td>1.202613</td>\n      <td>0.191881</td>\n      <td>0.272708</td>\n      <td>-0.005159</td>\n      <td>0.081213</td>\n      <td>0.464960</td>\n      <td>...</td>\n      <td>-0.167716</td>\n      <td>-0.270710</td>\n      <td>-0.154104</td>\n      <td>-0.780055</td>\n      <td>0.750137</td>\n      <td>-0.257237</td>\n      <td>0.034507</td>\n      <td>0.005168</td>\n      <td>4.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>7.0</td>\n      <td>-0.644269</td>\n      <td>1.417964</td>\n      <td>1.074380</td>\n      <td>-0.492199</td>\n      <td>0.948934</td>\n      <td>0.428118</td>\n      <td>1.120631</td>\n      <td>-3.807864</td>\n      <td>0.615375</td>\n      <td>...</td>\n      <td>1.943465</td>\n      <td>-1.015455</td>\n      <td>0.057504</td>\n      <td>-0.649709</td>\n      <td>-0.415267</td>\n      <td>-0.051634</td>\n      <td>-1.206921</td>\n      <td>-1.085339</td>\n      <td>40.80</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>7.0</td>\n      <td>-0.894286</td>\n      <td>0.286157</td>\n      <td>-0.113192</td>\n      <td>-0.271526</td>\n      <td>2.669599</td>\n      <td>3.721818</td>\n      <td>0.370145</td>\n      <td>0.851084</td>\n      <td>-0.392048</td>\n      <td>...</td>\n      <td>-0.073425</td>\n      <td>-0.268092</td>\n      <td>-0.204233</td>\n      <td>1.011592</td>\n      <td>0.373205</td>\n      <td>-0.384157</td>\n      <td>0.011747</td>\n      <td>0.142404</td>\n      <td>93.20</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>9.0</td>\n      <td>-0.338262</td>\n      <td>1.119593</td>\n      <td>1.044367</td>\n      <td>-0.222187</td>\n      <td>0.499361</td>\n      <td>-0.246761</td>\n      <td>0.651583</td>\n      <td>0.069539</td>\n      <td>-0.736727</td>\n      <td>...</td>\n      <td>-0.246914</td>\n      <td>-0.633753</td>\n      <td>-0.120794</td>\n      <td>-0.385050</td>\n      <td>-0.069733</td>\n      <td>0.094199</td>\n      <td>0.246219</td>\n      <td>0.083076</td>\n      <td>3.68</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#概览数据，显示前10行\n",
    "data_original.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\nTime      284807 non-null float64\nV1        284807 non-null float64\nV2        284807 non-null float64\nV3        284807 non-null float64\nV4        284807 non-null float64\nV5        284807 non-null float64\nV6        284807 non-null float64\nV7        284807 non-null float64\nV8        284807 non-null float64\nV9        284807 non-null float64\nV10       284807 non-null float64\nV11       284807 non-null float64\nV12       284807 non-null float64\nV13       284807 non-null float64\nV14       284807 non-null float64\nV15       284807 non-null float64\nV16       284807 non-null float64\nV17       284807 non-null float64\nV18       284807 non-null float64\nV19       284807 non-null float64\nV20       284807 non-null float64\nV21       284807 non-null float64\nV22       284807 non-null float64\nV23       284807 non-null float64\nV24       284807 non-null float64\nV25       284807 non-null float64\nV26       284807 non-null float64\nV27       284807 non-null float64\nV28       284807 non-null float64\nAmount    284807 non-null float64\nClass     284807 non-null int64\ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n"
    }
   ],
   "source": [
    "#显示数据规模，各个特征的数据类型；查看各个特征下是否存在缺失值Null\n",
    "#根据Kaggle上该数据集的描述，以及本条代码的查看结果，该数据的所有特征均为数值类型，并且没有缺失值，因此，不需要进行one-hot编码，也不需要进行缺失值处理\n",
    "data_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Time      0\nV1        0\nV2        0\nV3        0\nV4        0\nV5        0\nV6        0\nV7        0\nV8        0\nV9        0\nV10       0\nV11       0\nV12       0\nV13       0\nV14       0\nV15       0\nV16       0\nV17       0\nV18       0\nV19       0\nV20       0\nV21       0\nV22       0\nV23       0\nV24       0\nV25       0\nV26       0\nV27       0\nV28       0\nAmount    0\nClass     0\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#统计各个特征下Null的数量，经过查看，我们发现该数据的确不存在缺失值，与info()的结果一致\n",
    "data_original.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "#显示整个数据集中缺失值Null的总数\n",
    "data_original.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Time      124592\nV1        275663\nV2        275663\nV3        275663\nV4        275663\nV5        275663\nV6        275663\nV7        275663\nV8        275663\nV9        275663\nV10       275663\nV11       275663\nV12       275663\nV13       275663\nV14       275663\nV15       275663\nV16       275663\nV17       275663\nV18       275663\nV19       275663\nV20       275663\nV21       275663\nV22       275663\nV23       275663\nV24       275663\nV25       275663\nV26       275663\nV27       275663\nV28       275663\nAmount     32767\nClass          2\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#显示各个特征包含的唯一值的数量\n",
    "data_original.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    284315\n1       492\nName: Class, dtype: int64"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "#显示数据集中各个类别的数量（标签的数量），我们发现两个类别的分布极不平衡\n",
    "data_original['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    99.827251\n1     0.172749\nName: Class, dtype: float64"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "#显示数据集中各个类别的百分比（标签的百分比），再次验证这是一个类别分布极不平衡的数据集\n",
    "data_original['Class'].value_counts(normalize = True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                Time            V1            V2            V3            V4  \\\ncount  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \nmean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \nstd     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \nmin         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \nmax    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n\n                 V5            V6            V7            V8            V9  \\\ncount  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \nmean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \nstd    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \nmin   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \nmax    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n\n       ...           V21           V22           V23           V24  \\\ncount  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \nmean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \nstd    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \nmin    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \nmax    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n\n                V25           V26           V27           V28         Amount  \\\ncount  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \nmean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \nstd    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \nmin   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \nmax    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n\n               Class  \ncount  284807.000000  \nmean        0.001727  \nstd         0.041527  \nmin         0.000000  \n25%         0.000000  \n50%         0.000000  \n75%         0.000000  \nmax         1.000000  \n\n[8 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>count</td>\n      <td>284807.000000</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>...</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>284807.000000</td>\n      <td>284807.000000</td>\n    </tr>\n    <tr>\n      <td>mean</td>\n      <td>94813.859575</td>\n      <td>3.919560e-15</td>\n      <td>5.688174e-16</td>\n      <td>-8.769071e-15</td>\n      <td>2.782312e-15</td>\n      <td>-1.552563e-15</td>\n      <td>2.010663e-15</td>\n      <td>-1.694249e-15</td>\n      <td>-1.927028e-16</td>\n      <td>-3.137024e-15</td>\n      <td>...</td>\n      <td>1.537294e-16</td>\n      <td>7.959909e-16</td>\n      <td>5.367590e-16</td>\n      <td>4.458112e-15</td>\n      <td>1.453003e-15</td>\n      <td>1.699104e-15</td>\n      <td>-3.660161e-16</td>\n      <td>-1.206049e-16</td>\n      <td>88.349619</td>\n      <td>0.001727</td>\n    </tr>\n    <tr>\n      <td>std</td>\n      <td>47488.145955</td>\n      <td>1.958696e+00</td>\n      <td>1.651309e+00</td>\n      <td>1.516255e+00</td>\n      <td>1.415869e+00</td>\n      <td>1.380247e+00</td>\n      <td>1.332271e+00</td>\n      <td>1.237094e+00</td>\n      <td>1.194353e+00</td>\n      <td>1.098632e+00</td>\n      <td>...</td>\n      <td>7.345240e-01</td>\n      <td>7.257016e-01</td>\n      <td>6.244603e-01</td>\n      <td>6.056471e-01</td>\n      <td>5.212781e-01</td>\n      <td>4.822270e-01</td>\n      <td>4.036325e-01</td>\n      <td>3.300833e-01</td>\n      <td>250.120109</td>\n      <td>0.041527</td>\n    </tr>\n    <tr>\n      <td>min</td>\n      <td>0.000000</td>\n      <td>-5.640751e+01</td>\n      <td>-7.271573e+01</td>\n      <td>-4.832559e+01</td>\n      <td>-5.683171e+00</td>\n      <td>-1.137433e+02</td>\n      <td>-2.616051e+01</td>\n      <td>-4.355724e+01</td>\n      <td>-7.321672e+01</td>\n      <td>-1.343407e+01</td>\n      <td>...</td>\n      <td>-3.483038e+01</td>\n      <td>-1.093314e+01</td>\n      <td>-4.480774e+01</td>\n      <td>-2.836627e+00</td>\n      <td>-1.029540e+01</td>\n      <td>-2.604551e+00</td>\n      <td>-2.256568e+01</td>\n      <td>-1.543008e+01</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>25%</td>\n      <td>54201.500000</td>\n      <td>-9.203734e-01</td>\n      <td>-5.985499e-01</td>\n      <td>-8.903648e-01</td>\n      <td>-8.486401e-01</td>\n      <td>-6.915971e-01</td>\n      <td>-7.682956e-01</td>\n      <td>-5.540759e-01</td>\n      <td>-2.086297e-01</td>\n      <td>-6.430976e-01</td>\n      <td>...</td>\n      <td>-2.283949e-01</td>\n      <td>-5.423504e-01</td>\n      <td>-1.618463e-01</td>\n      <td>-3.545861e-01</td>\n      <td>-3.171451e-01</td>\n      <td>-3.269839e-01</td>\n      <td>-7.083953e-02</td>\n      <td>-5.295979e-02</td>\n      <td>5.600000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>50%</td>\n      <td>84692.000000</td>\n      <td>1.810880e-02</td>\n      <td>6.548556e-02</td>\n      <td>1.798463e-01</td>\n      <td>-1.984653e-02</td>\n      <td>-5.433583e-02</td>\n      <td>-2.741871e-01</td>\n      <td>4.010308e-02</td>\n      <td>2.235804e-02</td>\n      <td>-5.142873e-02</td>\n      <td>...</td>\n      <td>-2.945017e-02</td>\n      <td>6.781943e-03</td>\n      <td>-1.119293e-02</td>\n      <td>4.097606e-02</td>\n      <td>1.659350e-02</td>\n      <td>-5.213911e-02</td>\n      <td>1.342146e-03</td>\n      <td>1.124383e-02</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>75%</td>\n      <td>139320.500000</td>\n      <td>1.315642e+00</td>\n      <td>8.037239e-01</td>\n      <td>1.027196e+00</td>\n      <td>7.433413e-01</td>\n      <td>6.119264e-01</td>\n      <td>3.985649e-01</td>\n      <td>5.704361e-01</td>\n      <td>3.273459e-01</td>\n      <td>5.971390e-01</td>\n      <td>...</td>\n      <td>1.863772e-01</td>\n      <td>5.285536e-01</td>\n      <td>1.476421e-01</td>\n      <td>4.395266e-01</td>\n      <td>3.507156e-01</td>\n      <td>2.409522e-01</td>\n      <td>9.104512e-02</td>\n      <td>7.827995e-02</td>\n      <td>77.165000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>max</td>\n      <td>172792.000000</td>\n      <td>2.454930e+00</td>\n      <td>2.205773e+01</td>\n      <td>9.382558e+00</td>\n      <td>1.687534e+01</td>\n      <td>3.480167e+01</td>\n      <td>7.330163e+01</td>\n      <td>1.205895e+02</td>\n      <td>2.000721e+01</td>\n      <td>1.559499e+01</td>\n      <td>...</td>\n      <td>2.720284e+01</td>\n      <td>1.050309e+01</td>\n      <td>2.252841e+01</td>\n      <td>4.584549e+00</td>\n      <td>7.519589e+00</td>\n      <td>3.517346e+00</td>\n      <td>3.161220e+01</td>\n      <td>3.384781e+01</td>\n      <td>25691.160000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "#显示各个特征的一些统计学特征\n",
    "data_original.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据探索总结:  \n",
    "1. 数据中一共包含284807个样本  \n",
    "2. 数据一共包含30个特征列以及一个标签列  \n",
    "3. 数据的特征列和标签列下均没有缺失值，不需要进行缺失值处理  \n",
    "4. 数据的30个特征均为连续的数值特征(continuous numerical features)，没有类别特征(categorical features),不需要进行one-hot编码  \n",
    "5. 数据中两个类别（正常及欺诈）分布极不平衡，正常数据（非欺诈数据）所占比例为99.83%，欺诈数据所占比例为0.17%,因此这是一个非均衡数据集(imbalanced data)的分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. 数据集特征列和标签列的分离，训练集和测试集的分割  \n",
    ">为了尽可能避免数据信息泄露的问题，在对数据进行任何前处理之前，一定要先对数据进行训练集和测试集的分割  \n",
    ">[参考1：Normalize data before or after split of training and testing data?](https://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data)  \n",
    ">[参考2：Onehotencoding before or after split of training and testing data?](https://stackoverflow.com/questions/55525195/do-i-have-to-do-one-hot-encoding-separately-for-train-and-test-dataset)  \n",
    ">[参考3：Imputation before or after train test spliting](https://stats.stackexchange.com/questions/95083/imputation-before-or-after-splitting-into-train-and-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#首先对数据集进行特征，标签的分离\n",
    "X = data_original.iloc[:,0:-1]\n",
    "y = data_original['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...       V20       V21       V22       V23       V24  \\\n0  0.098698  0.363787  ...  0.251412 -0.018307  0.277838 -0.110474  0.066928   \n1  0.085102 -0.255425  ... -0.069083 -0.225775 -0.638672  0.101288 -0.339846   \n2  0.247676 -1.514654  ...  0.524980  0.247998  0.771679  0.909412 -0.689281   \n3  0.377436 -1.387024  ... -0.208038 -0.108300  0.005274 -0.190321 -1.175575   \n4 -0.270533  0.817739  ...  0.408542 -0.009431  0.798278 -0.137458  0.141267   \n\n        V25       V26       V27       V28  Amount  \n0  0.128539 -0.189115  0.133558 -0.021053  149.62  \n1  0.167170  0.125895 -0.008983  0.014724    2.69  \n2 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n3  0.647376 -0.221929  0.062723  0.061458  123.50  \n4 -0.206010  0.502292  0.219422  0.215153   69.99  \n\n[5 rows x 30 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>0.251412</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.069083</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.524980</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.208038</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>0.408542</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 30 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    0\n1    0\n2    0\n3    0\n4    0\nName: Class, dtype: int64"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#然后进行训练集和测试集的分割,75%训练集，25%测试集\n",
    "#注意，这里我们要采用分层抽样(stratified sampling)的方法，以保证训练集和测试集中类别的比例和总体数据类别的比例基本一致\n",
    "#此外，如果我们提前获知数据中的某一个特征是关键特征，那么在进行分层抽样的时候，也可以该特征的比例作为参考，进行抽样\n",
    "#参考链接：https://medium.com/@411.codebrain/train-test-split-vs-stratifiedshufflesplit-374c3dbdcc36\n",
    "#参考链接：https://zhuanlan.zhihu.com/p/49991313\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    213236\n1       369\nName: Class, dtype: int64"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "# 显示训练集中各个类别的数量，用于计算scale_pos_weight\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_pos_weight = sum(negative instances) / sum(positive instances)\n",
    "# 这个参数是XGBoost和LightGBM两个模型在对非均衡数据进行分类时用于控制类别平衡的最关键的参数\n",
    "# 实际应用时，也可以考虑使用按上述公式计算得到的值的平方根\n",
    "# 参考1：https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "# 参考2：https://stats.stackexchange.com/questions/243207/what-is-the-proper-usage-of-scale-pos-weight-in-xgboost-for-imbalanced-datasets\n",
    "scale_pos_weight_1 = 213236 / 369\n",
    "scale_pos_weight_2 = np.sqrt(scale_pos_weight_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "577.8753387533875"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "scale_pos_weight_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "24.039037808393818"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "scale_pos_weight_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. 对于非均衡数据集（imbalanced data）的处理  \n",
    ">对于非均衡数据的处理，有多种思路:  \n",
    ">* **重采样**，包括上采样（Oversampling,也叫过采样）和下采样（Undersampling,也叫欠采样），其基本思路就是将数据中两类的数量调整均衡一些，  \n",
    "让少的变多,让多的变少,从而使非均衡数据变得均衡.在重采样以后，再利用各种机器学习分类模型对数据进行分类。  \n",
    ">>- 两种重采样各自的实现方式均有很多种，实践中，我们利用[imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.under_sampling)库来完成各种重采样的实现  \n",
    ">* **利用imbalaced-learn中的分类器**, 这些分类器具有处理非均衡数据的内在机制,比如[BalancedRandomForestClassifier](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.ensemble.BalancedRandomForestClassifier.html#)和[RUSBoostClassifier](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.ensemble.RUSBoostClassifier.html)  \n",
    ">* **利用XGBoost和LightGBM**,这两种基于GBDT的强分类器，均可以设置参数'scale_pos_weight'来处理这种非均衡数据  \n",
    ">>- scale_pos_weight = number of negative samples / number of positive samples  \n",
    ">>- 对于二分类问题，正例(positive)用1表示，反例(negative)用0表示  \n",
    ">* 模型评估准则(Metrics)：对于非均衡数据，不能再使用accuracy作为评估准则，可以考虑使用f1_score或者专门针对非均衡问题的评估准则，比如  \n",
    "[geometric_mean_score](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.metrics.geometric_mean_score.html#imblearn.metrics.geometric_mean_score)  \n",
    "\n",
    ">* [参考1:Dealing With Class Imbalanced Datasets For Classification](https://towardsdatascience.com/dealing-with-class-imbalanced-datasets-for-classification-2cc6fad99fd9)    \n",
    ">* [参考2:机器学习之类别不平衡问题 (3) —— 采样方法](https://www.cnblogs.com/massquantity/p/9382710.html)  \n",
    ">* [参考3:机器学习中的非均衡问题(imbalanced data)和应对方法](https://zhuanlan.zhihu.com/p/38687978)  \n",
    ">* [参考4:机器学习：如何解决机器学习中数据不平衡问题](https://www.jianshu.com/p/be343414dd24)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. 训练模型并使用[Hyperopt](http://hyperopt.github.io/hyperopt/)进行超参数的调优  \n",
    ">* hyperopt是一种通过**贝叶斯优化(Bayesian Optimization)**来调整参数的工具  \n",
    ">* 三种调参方法GridSearch,RandomSearch以及Bayesian Search的对比可参见：  \n",
    ">>* [Intuitive Hyperparameter Optimization : Grid Search, Random Search and Bayesian Search](https://towardsdatascience.com/intuitive-hyperparameter-optimization-grid-search-random-search-and-bayesian-search-2102dbfaf5b)  \n",
    ">>* [贝叶斯优化: 一种更好的超参数调优方式](https://zhuanlan.zhihu.com/p/29779000)  \n",
    ">* 本项目采用十折交叉验证法进行参数调优,模型评估准则采用专门针对非均衡数据的准则geometric_mean_score  \n",
    ">* 思路：  \n",
    ">>1. 首先对未进行特征缩放的数据进行训练和测试，查看结果  \n",
    ">>2. 然后对经过特征缩放但未经过特征选择的数据进行训练和测试，查看结果  \n",
    ">>3. 最后对经过特征缩放和特征选择的数据进行训练和测试，查看结果  \n",
    ">* [RandomForest调参参考](https://www.cnblogs.com/pinard/p/6160412.html)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from sklearn.metrics import make_scorer\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from hyperopt import fmin, tpe, atpe, hp, STATUS_OK, Trials, space_eval # (atpe) adaptive TPE 算法是hyperopt最新版本加入的新算法\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对于非均衡数据的class_weight的计算方法\n",
    "#参考1：https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53696\n",
    "#参考2：https://stackoverflow.com/questions/44716150/how-can-i-assign-a-class-weight-in-keras-in-a-simple-way\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',classes = np.unique(y_train),y = y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "type(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([  0.50086524, 289.43766938])"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "type(class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{0: 0.5008652385150725, 1: 289.43766937669375}"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100%|██████████| 30/30 [36:47<00:00, 73.58s/it, best loss: -0.9374411453583523]\n{'class_weight': {0: 0.5008652385150725, 1: 289.43766937669375}, 'max_depth': 65, 'max_features': 3, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42, 'warm_start': True}\n-0.9374411453583523\nTraining time: 2207.403s\n"
    }
   ],
   "source": [
    "#BalancedRandomForest分类器的训练\n",
    "start = time.time()\n",
    "def brdf(params):\n",
    "    # ip = params[\"imputer\"]\n",
    "    # del params[\"imputer\"]\n",
    "    # sc = params[\"scaler\"]\n",
    "    # del params[\"scaler\"]\n",
    "    brdf_clf = BalancedRandomForestClassifier(**params)\n",
    "    str_kfold = StratifiedKFold(\n",
    "        n_splits=10, shuffle=True, random_state=42\n",
    "    )  # 注意随机数random_state保持一致，以便复现结果\n",
    "    # 参考链接：https://stackoverflow.com/questions/39782243/how-to-use-cross-val-score-with-random-state\n",
    "    gms = make_scorer(geometric_mean_score)\n",
    "    metric = cross_val_score(\n",
    "        brdf_clf,\n",
    "        # Data_to_opt(sc)[0],\n",
    "        # Data_to_opt(sc)[1],\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=str_kfold,\n",
    "        scoring = gms, \n",
    "        n_jobs=-1,  \n",
    "    ).mean()  \n",
    "    return {\"loss\": -metric, \"status\": STATUS_OK}\n",
    "\n",
    "space4brdf = {\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", range(100, 320, 20)),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", range(1, 70)),  #! max_depth 影响模型的复杂程度\n",
    "    # \"max_features\": 1,\n",
    "    \"max_features\": hp.choice(\"max_features\", range(1, 30)),\n",
    "    \"class_weight\": class_weight_dict,\n",
    "    \"warm_start\": True,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,  # 注意保持随机状态的一致性，以便复现结果\n",
    "    #\"imputation\": hp.choice(\"imputation\", [\"dropna\", \"SI\", \"MI\"]),\n",
    "    #\"scaling_method\": hp.choice(\"scaling_method\", [\"min_max\", \"std\"]),\n",
    "}\n",
    "\n",
    "rstate = np.random.RandomState(42)\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    brdf, space4brdf, algo=tpe.suggest, max_evals=30, trials=trials, rstate=rstate\n",
    ")  #! fmin返回的是这些最佳参数在其列表中的索引，而不是直接返回最佳参数本身\n",
    "# print(best)\n",
    "print(space_eval(space4brdf, best))  #! space_eval()输出最佳参数本身而不是索引\n",
    "# print(lgt(best))\n",
    "print(trials.best_trial[\"result\"][\"loss\"])\n",
    "# print(trials.best_trial[\"result\"])\n",
    "\n",
    "# 把最终搜索到的最有超参数写入到一个json文件\n",
    "# 参考链接: https://stackabuse.com/scikit-learn-save-and-restore-models/\n",
    "with open(\"brdf.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\"f1\": trials.best_trial[\"result\"][\"loss\"], \"Best params\": space_eval(space4brdf, best)}))\n",
    "hyperparams_brdf = space_eval(space4brdf, best)\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100%|██████████| 30/30 [1:12:24<00:00, 144.81s/it, best loss: -0.9396325585633679]\n{'learning_rate': 0.12470065927231533, 'n_estimators': 270, 'random_state': 42}\n-0.9396325585633679\nTraining time: 4344.322s\n"
    }
   ],
   "source": [
    "#RUSBoostClassifier的训练及参数调优\n",
    "start = time.time()\n",
    "def rusb(params):\n",
    "    # ip = params[\"imputer\"]\n",
    "    # del params[\"imputer\"]\n",
    "    # sc = params[\"scaler\"]\n",
    "    # del params[\"scaler\"]\n",
    "    rusb_clf = RUSBoostClassifier(**params)\n",
    "    str_kfold = StratifiedKFold(\n",
    "        n_splits=10, shuffle=True, random_state=42\n",
    "    )  # 注意随机数random_state保持一致，以便复现结果\n",
    "    # 参考链接：https://stackoverflow.com/questions/39782243/how-to-use-cross-val-score-with-random-state\n",
    "    gms = make_scorer(geometric_mean_score)\n",
    "    metric = cross_val_score(\n",
    "        rusb_clf,\n",
    "        # Data_to_opt(sc)[0],\n",
    "        # Data_to_opt(sc)[1],\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=str_kfold,\n",
    "        scoring = gms, \n",
    "        n_jobs=-1,  \n",
    "    ).mean()  \n",
    "    return {\"loss\": -metric, \"status\": STATUS_OK}\n",
    "\n",
    "space4rusb = {\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", range(50, 320, 20)),\n",
    "    # \"max_depth\": hp.choice(\"max_depth\", range(1, 70)),  \n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0, 1),\n",
    "    # \"max_features\": 1,\n",
    "    # \"max_features\": hp.choice(\"max_features\", range(1, 30)),\n",
    "    # \"class_weight\": class_weight_dict,\n",
    "    # \"warm_start\": True,\n",
    "    # \"n_jobs\": -1,\n",
    "    \"random_state\": 42,  \n",
    "    #\"imputation\": hp.choice(\"imputation\", [\"dropna\", \"SI\", \"MI\"]),\n",
    "    #\"scaling_method\": hp.choice(\"scaling_method\", [\"min_max\", \"std\"]),\n",
    "}\n",
    "\n",
    "rstate = np.random.RandomState(42)\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    rusb, space4rusb, algo=tpe.suggest, max_evals=30, trials=trials, rstate=rstate\n",
    ")  #! fmin返回的是这些最佳参数在其列表中的索引，而不是直接返回最佳参数本身\n",
    "# print(best)\n",
    "print(space_eval(space4rusb, best))  #! space_eval()输出最佳参数本身而不是索引\n",
    "# print(lgt(best))\n",
    "print(trials.best_trial[\"result\"][\"loss\"])\n",
    "# print(trials.best_trial[\"result\"])\n",
    "\n",
    "with open(\"rusb.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\"f1\": trials.best_trial[\"result\"][\"loss\"], \"Best params\": space_eval(space4rusb, best)}))\n",
    "hyperparams_rusb = space_eval(space4rusb, best)\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100%|██████████| 30/30 [20:17<00:00, 40.57s/it, best loss: -0.9307152801313322]\n{'colsample_bytree': 0.39779512373111203, 'gamma': 5, 'learning_rate': 0.326749198175752, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 50, 'n_jobs': -1, 'objective': 'binary:logistic', 'random_state': 42, 'scale_pos_weight': 577.8760162601626, 'subsample': 0.625032357077646, 'tree_method': 'hist'}\n-0.9307152801313322\nTraining time: 1217.138s\n"
    }
   ],
   "source": [
    "#XGBoostClassifier的训练和参数调优\n",
    "start = time.time()\n",
    "def xgb(params):\n",
    "    # ip = params[\"imputer\"]\n",
    "    # del params[\"imputer\"]\n",
    "    # sc = params[\"scaler\"]\n",
    "    # del params[\"scaler\"]\n",
    "    xgb_clf = XGBClassifier(**params)\n",
    "    str_kfold = StratifiedKFold(\n",
    "        n_splits=10, shuffle=True, random_state=42\n",
    "    )  #!Here the random state should be the same as that in the model\n",
    "    # ?https://stackoverflow.com/questions/39782243/how-to-use-cross-val-score-with-random-state\n",
    "    gms = make_scorer(geometric_mean_score)\n",
    "    metric = cross_val_score(\n",
    "        xgb_clf,\n",
    "        # Data_to_opt(sc)[0],\n",
    "        # Data_to_opt(sc)[1],\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=str_kfold,\n",
    "        scoring = gms, \n",
    "        n_jobs=-1,  \n",
    "    ).mean()  \n",
    "    return {\"loss\": -metric, \"status\": STATUS_OK}\n",
    "\n",
    "space4xgb = {\n",
    "    \"max_depth\": hp.choice(\"max_depth\", range(3, 20)),  \n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0, 1),\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", [50, 100, 150, 200, 250, 300]),\n",
    "    # \"objective\": \"multi:softmax\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"scale_pos_weight\": hp.choice(\"scale_pos_weight\", [scale_pos_weight_1,scale_pos_weight_2]),\n",
    "    \"n_jobs\": -1,\n",
    "    \"gamma\": hp.randint(\"gamma\", 10),\n",
    "    \"min_child_weight\": hp.choice(\"min_child_weight\", range(1, 10)),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.1, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.1, 1.0),\n",
    "    \"random_state\": 42,  \n",
    "    \"tree_method\": \"hist\",\n",
    "    # \"imputation\": hp.choice(\"imputation\", [\"dropna\", \"SI\", \"MI\"]),\n",
    "    # \"scaling_method\": hp.choice(\"scaling_method\", [\"min_max\", \"std\"]),\n",
    "}\n",
    "\n",
    "rstate = np.random.RandomState(42)\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    xgb, space4xgb, algo=tpe.suggest, max_evals=30, trials=trials, rstate=rstate\n",
    ")  #! fmin返回的是这些最佳参数在其列表中的索引，而不是直接返回最佳参数本身\n",
    "# print(best)\n",
    "print(space_eval(space4xgb, best))  #! space_eval()输出最佳参数本身而不是索引\n",
    "# print(lgt(best))\n",
    "print(trials.best_trial[\"result\"][\"loss\"])\n",
    "# print(trials.best_trial[\"result\"])\n",
    "\n",
    "\n",
    "with open(\"xgb.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\"f1\": trials.best_trial[\"result\"][\"loss\"], \"Best params\": space_eval(space4xgb, best)}))\n",
    "hyperparams_xgb =  space_eval(space4xgb, best)\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100%|██████████| 30/30 [14:27<00:00, 28.90s/it, best loss: -0.920832927590423]\n{'colsample_bytree': 0.6688200948328725, 'device': 'gpu', 'gpu_device_id': 0, 'gpu_platform_id': 0, 'gpu_use_dp': False, 'learning_rate': 0.01378363697590293, 'max_bin': 63, 'max_depth': 15, 'min_child_samples': 17, 'min_child_weight': 9.842579154419157, 'min_split_gain': 49.99706400365879, 'n_jobs': -1, 'num_boost_round': 499, 'num_leaves': 266, 'objective': 'binary', 'random_state': 42, 'scale_pos_weight': 577.8753387533875, 'subsample': 0.9905036472418532, 'subsample_freq': 18}\n-0.920832927590423\nTraining time: 867.386s\n"
    }
   ],
   "source": [
    "#LightGBMClassifier的训练和恶参数调优\n",
    "start = time.time()\n",
    "def lgbm(params):\n",
    "    # ip = params[\"imputer\"]\n",
    "    # del params[\"imputer\"]\n",
    "    # sc = params[\"scaler\"]\n",
    "    # del params[\"scaler\"]\n",
    "    lgbm_clf = LGBMClassifier(**params)\n",
    "    str_kfold = StratifiedKFold(\n",
    "        n_splits=10, shuffle=True, random_state=42\n",
    "    )  \n",
    "    gms = make_scorer(geometric_mean_score)\n",
    "    metric = cross_val_score(\n",
    "        lgbm_clf,\n",
    "        # Data_to_opt(sc)[0],\n",
    "        # Data_to_opt(sc)[1],\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=str_kfold,\n",
    "        scoring = gms, \n",
    "        n_jobs=-1,  \n",
    "    ).mean()  \n",
    "    return {\"loss\": -metric, \"status\": STATUS_OK}\n",
    "\n",
    "space4lgbm = {\n",
    "    # 参考链接： https://lightgbm.readthedocs.io/en/latest/Parameters.html#max_bin\n",
    "    # max_bin: int, default =255, >1,\n",
    "    # smaller max_bin, faster speed, maybe underfitting; larger max_bin, slower speed, maybe overfitting\n",
    "    \"max_bin\": 63,\n",
    "    \"num_leaves\": hp.choice(\"num_leaves\", range(100, 500)),  # * the larger this value, the more complex the model is\n",
    "    \"max_depth\": hp.choice(\"max_depth\", range(3, 32)),\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.2),\n",
    "    # \"n_estimators\": hp.choice(\"n_estimators\", [50, 100, 150, 200, 250, 300]), #! n_estimators has bug here\n",
    "    \"num_boost_round\": hp.choice(\"num_boost_round\", range(50, 500)),  #! this is an alias of n_estimators but no bug\n",
    "    # \"objective\": \"multiclass\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"n_jobs\": -1,\n",
    "    # \"class_weight\": \"balanced\",  #! This set should be done when the classes are imbalanced\n",
    "    \"scale_pos_weight\": hp.choice(\"scale_pos_weight\", [scale_pos_weight_1,scale_pos_weight_2]), #控制非均衡是数据中各个类别的平衡\n",
    "    \"min_split_gain\": hp.uniform(\"gamma\", 0, 50),  #! this is the 'gamma' in xgboost but its type is float now\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0, 10),\n",
    "    \"min_child_samples\": hp.randint(\"min_child_samples\", 20),  #! too large may cause underfitting\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.1, 1.0),\n",
    "    \"subsample_freq\": hp.choice(\"subsample_freq\", range(1, 30)),  #! k means perform bagging at every k iteration\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.1, 1.0),\n",
    "    \"random_state\": 42,  #! Here the random state should be the same as that in the stratify Kfold setttings\n",
    "    \"gpu_use_dp\": False,  #! for result's reproducibility\n",
    "    \"device\": \"gpu\",\n",
    "    \"gpu_platform_id\": 0,  # *OpenCL platform ID   小规模的CPU会快，大规模的GPU会快\n",
    "    \"gpu_device_id\": 0,  # *OpenCL device ID\n",
    "    # \"imputation\": hp.choice(\"imputation\", [\"dropna\", \"SI\", \"MI\"]),\n",
    "    # \"scaling_method\": hp.choice(\"scaling_method\", [\"min_max\", \"std\"]),\n",
    "}#!fmin needs this random state for reproducibility, and all the random seed should be the same as above.\n",
    "rstate = np.random.RandomState(42)\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    lgbm, space4lgbm, algo=tpe.suggest, max_evals=30, trials=trials, rstate=rstate\n",
    ")  #! fmin返回的是这些最佳参数在其列表中的索引，而不是直接返回最佳参数本身\n",
    "# print(best)\n",
    "print(space_eval(space4lgbm, best))  #! space_eval()输出最佳参数本身而不是索引\n",
    "# print(lgt(best))\n",
    "print(trials.best_trial[\"result\"][\"loss\"])\n",
    "# print(trials.best_trial[\"result\"])\n",
    "\n",
    "\n",
    "with open(\"lgbm.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\"f1\": trials.best_trial[\"result\"][\"loss\"], \"Best params\": space_eval(space4lgbm, best)}))\n",
    "hyperparams_lgbm = space_eval(space4lgbm, best)\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII.测试，检验模型的泛化能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BalancedRandomForestClassifier(bootstrap=True,\n                               class_weight={0: 0.5008652385150725,\n                                             1: 289.43766937669375},\n                               criterion='gini', max_depth=65, max_features=3,\n                               max_leaf_nodes=None, min_impurity_decrease=0.0,\n                               min_samples_leaf=2, min_samples_split=2,\n                               min_weight_fraction_leaf=0.0, n_estimators=300,\n                               n_jobs=-1, oob_score=False, random_state=42,\n                               replacement=False, sampling_strategy='auto',\n                               verbose=0, warm_start=True)"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "# BalancedRandomForestClassifier Refit\n",
    "brdf_refit = BalancedRandomForestClassifier(**hyperparams_brdf)\n",
    "brdf_refit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RUSBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n                   learning_rate=0.12470065927231533, n_estimators=270,\n                   random_state=42, replacement=False,\n                   sampling_strategy='auto')"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# RUSBoostClassifier Refit\n",
    "rusb_refit = RUSBoostClassifier(**hyperparams_rusb)\n",
    "rusb_refit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.39779512373111203, gamma=5,\n              learning_rate=0.326749198175752, max_delta_step=0, max_depth=3,\n              min_child_weight=2, missing=None, n_estimators=50, n_jobs=-1,\n              nthread=None, objective='binary:logistic', random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=577.8760162601626,\n              seed=None, silent=None, subsample=0.625032357077646,\n              tree_method='hist', verbosity=1)"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "# XGBoostClassifier Refit\n",
    "xgb_refit = XGBClassifier(**hyperparams_xgb)\n",
    "xgb_refit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LGBMClassifier(boosting_type='gbdt', class_weight=None,\n               colsample_bytree=0.6688200948328725, device='gpu',\n               gpu_device_id=0, gpu_platform_id=0, gpu_use_dp=False,\n               importance_type='split', learning_rate=0.01378363697590293,\n               max_bin=63, max_depth=15, min_child_samples=17,\n               min_child_weight=9.842579154419157,\n               min_split_gain=49.99706400365879, n_estimators=100, n_jobs=-1,\n               num_boost_round=499, num_leaves=266, objective='binary',\n               random_state=42, reg_alpha=0.0, reg_lambda=0.0,\n               scale_pos_weight=577.8753387533875, silent=True,\n               subsample=0.9905036472418532, subsample_for_bin=200000,\n               subsample_freq=18)"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "# LGBMClassifier Refit\n",
    "lgbm_refit = LGBMClassifier(**hyperparams_lgbm)\n",
    "lgbm_refit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Final geometric_mean_score: 0.9281944766749752\n"
    }
   ],
   "source": [
    "# BalancedRandomForestClassifier Test\n",
    "y_test_pred = brdf_refit.predict(X_test)\n",
    "gms = geometric_mean_score(y_test, y_test_pred, average=\"binary\")  \n",
    "print(\"Final geometric_mean_score:\", gms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Final geometric_mean_score: 0.9306201600729969\n"
    }
   ],
   "source": [
    "# RUSBoostClassifier Test\n",
    "y_test_pred = rusb_refit.predict(X_test)\n",
    "gms = geometric_mean_score(y_test, y_test_pred, average=\"binary\")  \n",
    "print(\"Final geometric_mean_score:\", gms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Final geometric_mean_score: 0.9307896965954383\n"
    }
   ],
   "source": [
    "# XGBoostClassifier Test\n",
    "y_test_pred = xgb_refit.predict(X_test)\n",
    "gms = geometric_mean_score(y_test, y_test_pred, average=\"binary\")  \n",
    "print(\"Final geometric_mean_score:\", gms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Final geometric_mean_score: 0.931538876124443\n"
    }
   ],
   "source": [
    "# LGBMClassifier Test\n",
    "y_test_pred = lgbm_refit.predict(X_test)\n",
    "gms = geometric_mean_score(y_test, y_test_pred, average=\"binary\")  \n",
    "print(\"Final geometric_mean_score:\", gms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitb5a08d408f8d409489ba603a7485d697",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}